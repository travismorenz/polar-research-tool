<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Key Phrase: LDA</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>adversarial example</td>
      <td>0.029274</td>
    </tr>
    <tr>
      <th>1</th>
      <td>neural network</td>
      <td>0.028611</td>
    </tr>
    <tr>
      <th>2</th>
      <td>adversarial attack</td>
      <td>0.015720</td>
    </tr>
    <tr>
      <th>3</th>
      <td>state art</td>
      <td>0.012850</td>
    </tr>
    <tr>
      <th>4</th>
      <td>adversarial training</td>
      <td>0.011924</td>
    </tr>
    <tr>
      <th>5</th>
      <td>deep neural</td>
      <td>0.011518</td>
    </tr>
    <tr>
      <th>6</th>
      <td>deep neural network</td>
      <td>0.011345</td>
    </tr>
    <tr>
      <th>7</th>
      <td>adversarial perturbation</td>
      <td>0.010350</td>
    </tr>
    <tr>
      <th>8</th>
      <td>black box</td>
      <td>0.007992</td>
    </tr>
    <tr>
      <th>9</th>
      <td>deep learning</td>
      <td>0.005933</td>
    </tr>
    <tr>
      <th>10</th>
      <td>vulnerable adversarial</td>
      <td>0.005283</td>
    </tr>
    <tr>
      <th>11</th>
      <td>adversarial image</td>
      <td>0.004791</td>
    </tr>
    <tr>
      <th>12</th>
      <td>network dnn</td>
      <td>0.004085</td>
    </tr>
    <tr>
      <th>13</th>
      <td>neural network dnn</td>
      <td>0.004085</td>
    </tr>
    <tr>
      <th>14</th>
      <td>machine learning</td>
      <td>0.004001</td>
    </tr>
    <tr>
      <th>15</th>
      <td>box attack</td>
      <td>0.003963</td>
    </tr>
    <tr>
      <th>16</th>
      <td>generate adversarial</td>
      <td>0.003920</td>
    </tr>
    <tr>
      <th>17</th>
      <td>paper propose</td>
      <td>0.003645</td>
    </tr>
    <tr>
      <th>18</th>
      <td>adversarial sample</td>
      <td>0.003638</td>
    </tr>
    <tr>
      <th>19</th>
      <td>machine learn</td>
      <td>0.003612</td>
    </tr>
    <tr>
      <th>20</th>
      <td>mnist cifar</td>
      <td>0.003587</td>
    </tr>
    <tr>
      <th>21</th>
      <td>attack method</td>
      <td>0.003490</td>
    </tr>
    <tr>
      <th>22</th>
      <td>improve robustness</td>
      <td>0.003421</td>
    </tr>
    <tr>
      <th>23</th>
      <td>convolutional neural</td>
      <td>0.003380</td>
    </tr>
    <tr>
      <th>24</th>
      <td>convolutional neural network</td>
      <td>0.003380</td>
    </tr>
    <tr>
      <th>25</th>
      <td>propose method</td>
      <td>0.003355</td>
    </tr>
    <tr>
      <th>26</th>
      <td>image classification</td>
      <td>0.003159</td>
    </tr>
    <tr>
      <th>27</th>
      <td>learning model</td>
      <td>0.003145</td>
    </tr>
    <tr>
      <th>28</th>
      <td>training datum</td>
      <td>0.003083</td>
    </tr>
    <tr>
      <th>29</th>
      <td>white box</td>
      <td>0.003022</td>
    </tr>
    <tr>
      <th>30</th>
      <td>black box attack</td>
      <td>0.002725</td>
    </tr>
    <tr>
      <th>31</th>
      <td>real world</td>
      <td>0.002688</td>
    </tr>
    <tr>
      <th>32</th>
      <td>network adversarial</td>
      <td>0.002580</td>
    </tr>
    <tr>
      <th>33</th>
      <td>small perturbation</td>
      <td>0.002394</td>
    </tr>
    <tr>
      <th>34</th>
      <td>train model</td>
      <td>0.002393</td>
    </tr>
    <tr>
      <th>35</th>
      <td>adversarial network</td>
      <td>0.002388</td>
    </tr>
    <tr>
      <th>36</th>
      <td>vulnerable adversarial example</td>
      <td>0.002378</td>
    </tr>
    <tr>
      <th>37</th>
      <td>base attack</td>
      <td>0.002340</td>
    </tr>
    <tr>
      <th>38</th>
      <td>generative adversarial</td>
      <td>0.002282</td>
    </tr>
    <tr>
      <th>39</th>
      <td>success rate</td>
      <td>0.002245</td>
    </tr>
    <tr>
      <th>40</th>
      <td>art attack</td>
      <td>0.002238</td>
    </tr>
    <tr>
      <th>41</th>
      <td>experimental result</td>
      <td>0.002203</td>
    </tr>
    <tr>
      <th>42</th>
      <td>perturbation input</td>
      <td>0.002150</td>
    </tr>
    <tr>
      <th>43</th>
      <td>generative adversarial network</td>
      <td>0.002071</td>
    </tr>
    <tr>
      <th>44</th>
      <td>adversarial input</td>
      <td>0.002070</td>
    </tr>
    <tr>
      <th>45</th>
      <td>computer vision</td>
      <td>0.002068</td>
    </tr>
    <tr>
      <th>46</th>
      <td>state art attack</td>
      <td>0.002067</td>
    </tr>
    <tr>
      <th>47</th>
      <td>deep network</td>
      <td>0.001979</td>
    </tr>
    <tr>
      <th>48</th>
      <td>recent work</td>
      <td>0.001919</td>
    </tr>
    <tr>
      <th>49</th>
      <td>result show</td>
      <td>0.001903</td>
    </tr>
    <tr>
      <th>50</th>
      <td>experiment show</td>
      <td>0.001898</td>
    </tr>
    <tr>
      <th>51</th>
      <td>adversarial robustness</td>
      <td>0.001852</td>
    </tr>
    <tr>
      <th>52</th>
      <td>example generate</td>
      <td>0.001851</td>
    </tr>
    <tr>
      <th>53</th>
      <td>propose defense</td>
      <td>0.001828</td>
    </tr>
    <tr>
      <th>54</th>
      <td>generate adversarial example</td>
      <td>0.001817</td>
    </tr>
    <tr>
      <th>55</th>
      <td>adversarial example generate</td>
      <td>0.001811</td>
    </tr>
    <tr>
      <th>56</th>
      <td>robustness adversarial</td>
      <td>0.001725</td>
    </tr>
    <tr>
      <th>57</th>
      <td>deep learn</td>
      <td>0.001688</td>
    </tr>
    <tr>
      <th>58</th>
      <td>machine learning model</td>
      <td>0.001686</td>
    </tr>
    <tr>
      <th>59</th>
      <td>network train</td>
      <td>0.001686</td>
    </tr>
    <tr>
      <th>60</th>
      <td>increase robustness</td>
      <td>0.001678</td>
    </tr>
    <tr>
      <th>61</th>
      <td>input image</td>
      <td>0.001677</td>
    </tr>
    <tr>
      <th>62</th>
      <td>neural network adversarial</td>
      <td>0.001676</td>
    </tr>
    <tr>
      <th>63</th>
      <td>object detection</td>
      <td>0.001668</td>
    </tr>
    <tr>
      <th>64</th>
      <td>extensive experiment</td>
      <td>0.001664</td>
    </tr>
    <tr>
      <th>65</th>
      <td>network architecture</td>
      <td>0.001657</td>
    </tr>
    <tr>
      <th>66</th>
      <td>train neural</td>
      <td>0.001652</td>
    </tr>
    <tr>
      <th>67</th>
      <td>imperceptible perturbation</td>
      <td>0.001618</td>
    </tr>
    <tr>
      <th>68</th>
      <td>defense strategy</td>
      <td>0.001617</td>
    </tr>
    <tr>
      <th>69</th>
      <td>training set</td>
      <td>0.001580</td>
    </tr>
    <tr>
      <th>70</th>
      <td>pre train</td>
      <td>0.001574</td>
    </tr>
    <tr>
      <th>71</th>
      <td>detect adversarial</td>
      <td>0.001551</td>
    </tr>
    <tr>
      <th>72</th>
      <td>input space</td>
      <td>0.001543</td>
    </tr>
    <tr>
      <th>73</th>
      <td>train neural network</td>
      <td>0.001536</td>
    </tr>
    <tr>
      <th>74</th>
      <td>target model</td>
      <td>0.001535</td>
    </tr>
    <tr>
      <th>75</th>
      <td>art performance</td>
      <td>0.001529</td>
    </tr>
    <tr>
      <th>76</th>
      <td>state art performance</td>
      <td>0.001529</td>
    </tr>
    <tr>
      <th>77</th>
      <td>vulnerable adversarial attack</td>
      <td>0.001519</td>
    </tr>
    <tr>
      <th>78</th>
      <td>box adversarial</td>
      <td>0.001509</td>
    </tr>
    <tr>
      <th>79</th>
      <td>generative model</td>
      <td>0.001490</td>
    </tr>
    <tr>
      <th>80</th>
      <td>benchmark dataset</td>
      <td>0.001465</td>
    </tr>
    <tr>
      <th>81</th>
      <td>image classifier</td>
      <td>0.001459</td>
    </tr>
    <tr>
      <th>82</th>
      <td>network show</td>
      <td>0.001446</td>
    </tr>
    <tr>
      <th>83</th>
      <td>network cnn</td>
      <td>0.001427</td>
    </tr>
    <tr>
      <th>84</th>
      <td>learn algorithm</td>
      <td>0.001419</td>
    </tr>
    <tr>
      <th>85</th>
      <td>high dimensional</td>
      <td>0.001404</td>
    </tr>
    <tr>
      <th>86</th>
      <td>deep convolutional</td>
      <td>0.001388</td>
    </tr>
    <tr>
      <th>87</th>
      <td>show vulnerable</td>
      <td>0.001379</td>
    </tr>
    <tr>
      <th>88</th>
      <td>neural network cnn</td>
      <td>0.001376</td>
    </tr>
    <tr>
      <th>89</th>
      <td>threat model</td>
      <td>0.001368</td>
    </tr>
    <tr>
      <th>90</th>
      <td>deep learning model</td>
      <td>0.001365</td>
    </tr>
    <tr>
      <th>91</th>
      <td>natural image</td>
      <td>0.001347</td>
    </tr>
    <tr>
      <th>92</th>
      <td>carefully craft</td>
      <td>0.001339</td>
    </tr>
    <tr>
      <th>93</th>
      <td>ser system</td>
      <td>0.001326</td>
    </tr>
    <tr>
      <th>94</th>
      <td>defense method</td>
      <td>0.001310</td>
    </tr>
    <tr>
      <th>95</th>
      <td>robust adversarial</td>
      <td>0.001303</td>
    </tr>
    <tr>
      <th>96</th>
      <td>bad case</td>
      <td>0.001301</td>
    </tr>
    <tr>
      <th>97</th>
      <td>achieve state art</td>
      <td>0.001295</td>
    </tr>
    <tr>
      <th>98</th>
      <td>achieve state</td>
      <td>0.001295</td>
    </tr>
    <tr>
      <th>99</th>
      <td>network adversarial example</td>
      <td>0.001287</td>
    </tr>
    <tr>
      <th>100</th>
      <td>adversarially train</td>
      <td>0.001283</td>
    </tr>
    <tr>
      <th>101</th>
      <td>defense mechanism</td>
      <td>0.001280</td>
    </tr>
    <tr>
      <th>102</th>
      <td>box adversarial attack</td>
      <td>0.001274</td>
    </tr>
    <tr>
      <th>103</th>
      <td>defense model</td>
      <td>0.001264</td>
    </tr>
    <tr>
      <th>104</th>
      <td>show vulnerable adversarial</td>
      <td>0.001263</td>
    </tr>
    <tr>
      <th>105</th>
      <td>original image</td>
      <td>0.001239</td>
    </tr>
    <tr>
      <th>106</th>
      <td>adversarial defense</td>
      <td>0.001235</td>
    </tr>
    <tr>
      <th>107</th>
      <td>imperceptible human</td>
      <td>0.001229</td>
    </tr>
    <tr>
      <th>108</th>
      <td>classification task</td>
      <td>0.001224</td>
    </tr>
    <tr>
      <th>109</th>
      <td>craft adversarial</td>
      <td>0.001220</td>
    </tr>
    <tr>
      <th>110</th>
      <td>white box attack</td>
      <td>0.001220</td>
    </tr>
    <tr>
      <th>111</th>
      <td>face recognition</td>
      <td>0.001212</td>
    </tr>
    <tr>
      <th>112</th>
      <td>gradient sign</td>
      <td>0.001202</td>
    </tr>
    <tr>
      <th>113</th>
      <td>step adversarial</td>
      <td>0.001200</td>
    </tr>
    <tr>
      <th>114</th>
      <td>fast gradient</td>
      <td>0.001195</td>
    </tr>
    <tr>
      <th>115</th>
      <td>model robustness</td>
      <td>0.001190</td>
    </tr>
    <tr>
      <th>116</th>
      <td>vulnerable adversarial perturbation</td>
      <td>0.001189</td>
    </tr>
    <tr>
      <th>117</th>
      <td>fast gradient sign</td>
      <td>0.001185</td>
    </tr>
    <tr>
      <th>118</th>
      <td>recent study</td>
      <td>0.001184</td>
    </tr>
    <tr>
      <th>119</th>
      <td>model adversarial</td>
      <td>0.001182</td>
    </tr>
    <tr>
      <th>120</th>
      <td>outperform state art</td>
      <td>0.001173</td>
    </tr>
    <tr>
      <th>121</th>
      <td>outperform state</td>
      <td>0.001173</td>
    </tr>
    <tr>
      <th>122</th>
      <td>significantly improve</td>
      <td>0.001170</td>
    </tr>
    <tr>
      <th>123</th>
      <td>simple effective</td>
      <td>0.001157</td>
    </tr>
    <tr>
      <th>124</th>
      <td>confidence attack</td>
      <td>0.001154</td>
    </tr>
    <tr>
      <th>125</th>
      <td>attack adversarial</td>
      <td>0.001152</td>
    </tr>
    <tr>
      <th>126</th>
      <td>loss function</td>
      <td>0.001151</td>
    </tr>
    <tr>
      <th>127</th>
      <td>attack algorithm</td>
      <td>0.001150</td>
    </tr>
    <tr>
      <th>128</th>
      <td>cifar imagenet</td>
      <td>0.001146</td>
    </tr>
    <tr>
      <th>129</th>
      <td>gradient descent</td>
      <td>0.001146</td>
    </tr>
    <tr>
      <th>130</th>
      <td>neural network show</td>
      <td>0.001128</td>
    </tr>
    <tr>
      <th>131</th>
      <td>find adversarial</td>
      <td>0.001127</td>
    </tr>
    <tr>
      <th>132</th>
      <td>attack scenario</td>
      <td>0.001125</td>
    </tr>
    <tr>
      <th>133</th>
      <td>robustness deep</td>
      <td>0.001117</td>
    </tr>
    <tr>
      <th>134</th>
      <td>recent year</td>
      <td>0.001107</td>
    </tr>
    <tr>
      <th>135</th>
      <td>safety critical</td>
      <td>0.001103</td>
    </tr>
    <tr>
      <th>136</th>
      <td>paper present</td>
      <td>0.001096</td>
    </tr>
    <tr>
      <th>137</th>
      <td>universal perturbation</td>
      <td>0.001093</td>
    </tr>
    <tr>
      <th>138</th>
      <td>propose approach</td>
      <td>0.001092</td>
    </tr>
    <tr>
      <th>139</th>
      <td>training method</td>
      <td>0.001087</td>
    </tr>
    <tr>
      <th>140</th>
      <td>large scale</td>
      <td>0.001081</td>
    </tr>
    <tr>
      <th>141</th>
      <td>art model</td>
      <td>0.001079</td>
    </tr>
    <tr>
      <th>142</th>
      <td>multi step</td>
      <td>0.001066</td>
    </tr>
    <tr>
      <th>143</th>
      <td>attack paper</td>
      <td>0.001064</td>
    </tr>
    <tr>
      <th>144</th>
      <td>network vulnerable</td>
      <td>0.001064</td>
    </tr>
    <tr>
      <th>145</th>
      <td>propose algorithm</td>
      <td>0.001063</td>
    </tr>
    <tr>
      <th>146</th>
      <td>detect adversarial example</td>
      <td>0.001063</td>
    </tr>
    <tr>
      <th>147</th>
      <td>robustness neural</td>
      <td>0.001054</td>
    </tr>
    <tr>
      <th>148</th>
      <td>decision boundary</td>
      <td>0.001052</td>
    </tr>
    <tr>
      <th>149</th>
      <td>learning system</td>
      <td>0.001051</td>
    </tr>
  </tbody>
</table>