<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Title</th>
      <th>Citations</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Intriguing properties of neural networks</td>
      <td>999</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Explaining and Harnessing Adversarial Examples</td>
      <td>999</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Deep Neural Networks are Easily Fooled: High Confidence Predictions for\n  Unrecognizable Images</td>
      <td>741</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Towards Evaluating the Robustness of Neural Networks</td>
      <td>641</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Adversarial examples in the physical world</td>
      <td>552</td>
    </tr>
    <tr>
      <th>5</th>
      <td>DeepFool: a simple and accurate method to fool deep neural networks</td>
      <td>517</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Distillation as a Defense to Adversarial Perturbations against Deep\n  Neural Networks</td>
      <td>490</td>
    </tr>
    <tr>
      <th>7</th>
      <td>The Limitations of Deep Learning in Adversarial Settings</td>
      <td>462</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Towards Deep Learning Models Resistant to Adversarial Attacks</td>
      <td>418</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Differentially Private Empirical Risk Minimization</td>
      <td>360</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Deep Learning with Differential Privacy</td>
      <td>294</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Universal adversarial perturbations</td>
      <td>279</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Ensemble Adversarial Training: Attacks and Defenses</td>
      <td>266</td>
    </tr>
    <tr>
      <th>13</th>
      <td>Adversarial Machine Learning at Scale</td>
      <td>265</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Obfuscated Gradients Give a False Sense of Security: Circumventing\n  Defenses to Adversarial Ex...</td>
      <td>239</td>
    </tr>
    <tr>
      <th>15</th>
      <td>Delving into Transferable Adversarial Examples and Black-box Attacks</td>
      <td>235</td>
    </tr>
    <tr>
      <th>16</th>
      <td>Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection\n  Methods</td>
      <td>231</td>
    </tr>
    <tr>
      <th>17</th>
      <td>Towards Deep Neural Network Architectures Robust to Adversarial Examples</td>
      <td>212</td>
    </tr>
    <tr>
      <th>18</th>
      <td>Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks</td>
      <td>185</td>
    </tr>
    <tr>
      <th>19</th>
      <td>Stealing Machine Learning Models via Prediction APIs</td>
      <td>181</td>
    </tr>
    <tr>
      <th>20</th>
      <td>Feature Squeezing: Detecting Adversarial Examples in Deep Neural\n  Networks</td>
      <td>172</td>
    </tr>
    <tr>
      <th>21</th>
      <td>On Detecting Adversarial Perturbations</td>
      <td>165</td>
    </tr>
    <tr>
      <th>22</th>
      <td>Adversarial Examples for Evaluating Reading Comprehension Systems</td>
      <td>163</td>
    </tr>
    <tr>
      <th>23</th>
      <td>Membership Inference Attacks against Machine Learning Models</td>
      <td>161</td>
    </tr>
    <tr>
      <th>24</th>
      <td>One pixel attack for fooling deep neural networks</td>
      <td>146</td>
    </tr>
    <tr>
      <th>25</th>
      <td>Synthesizing Robust Adversarial Examples</td>
      <td>145</td>
    </tr>
    <tr>
      <th>26</th>
      <td>Detecting Adversarial Samples from Artifacts</td>
      <td>145</td>
    </tr>
    <tr>
      <th>27</th>
      <td>Distributional Smoothing with Virtual Adversarial Training</td>
      <td>140</td>
    </tr>
    <tr>
      <th>28</th>
      <td>Unsupervised Anomaly Detection with Generative Adversarial Networks to\n  Guide Marker Discovery</td>
      <td>120</td>
    </tr>
    <tr>
      <th>29</th>
      <td>Differentially Private Empirical Risk Minimization: Efficient Algorithms\n  and Tight Error Bounds</td>
      <td>118</td>
    </tr>
    <tr>
      <th>30</th>
      <td>On the (Statistical) Detection of Adversarial Examples</td>
      <td>111</td>
    </tr>
    <tr>
      <th>31</th>
      <td>Certified Defenses against Adversarial Examples</td>
      <td>110</td>
    </tr>
    <tr>
      <th>32</th>
      <td>Countering Adversarial Images using Input Transformations</td>
      <td>108</td>
    </tr>
    <tr>
      <th>33</th>
      <td>Towards the Science of Security and Privacy in Machine Learning</td>
      <td>107</td>
    </tr>
    <tr>
      <th>34</th>
      <td>Analysis of classifiers' robustness to adversarial perturbations</td>
      <td>100</td>
    </tr>
    <tr>
      <th>35</th>
      <td>Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using\n  Generative Models</td>
      <td>100</td>
    </tr>
    <tr>
      <th>36</th>
      <td>The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</td>
      <td>99</td>
    </tr>
    <tr>
      <th>37</th>
      <td>Adversarial Attacks on Neural Network Policies</td>
      <td>96</td>
    </tr>
    <tr>
      <th>38</th>
      <td>Scribbler: Controlling Deep Image Synthesis with Sketch and Color</td>
      <td>96</td>
    </tr>
    <tr>
      <th>39</th>
      <td>Improving the Robustness of Deep Neural Networks via Stability Training</td>
      <td>94</td>
    </tr>
    <tr>
      <th>40</th>
      <td>Adversarial Examples for Semantic Segmentation and Object Detection</td>
      <td>89</td>
    </tr>
    <tr>
      <th>41</th>
      <td>Robustness of classifiers: from adversarial to random noise</td>
      <td>88</td>
    </tr>
    <tr>
      <th>42</th>
      <td>The Space of Transferable Adversarial Examples</td>
      <td>87</td>
    </tr>
    <tr>
      <th>43</th>
      <td>Threat of Adversarial Attacks on Deep Learning in Computer Vision: A\n  Survey</td>
      <td>83</td>
    </tr>
    <tr>
      <th>44</th>
      <td>Mitigating Adversarial Effects Through Randomization</td>
      <td>81</td>
    </tr>
    <tr>
      <th>45</th>
      <td>Measuring Neural Net Robustness with Constraints</td>
      <td>81</td>
    </tr>
    <tr>
      <th>46</th>
      <td>Defensive Distillation is Not Robust to Adversarial Examples</td>
      <td>80</td>
    </tr>
    <tr>
      <th>47</th>
      <td>Deep Models Under the GAN: Information Leakage from Collaborative Deep\n  Learning</td>
      <td>78</td>
    </tr>
    <tr>
      <th>48</th>
      <td>Adversarial Manipulation of Deep Representations</td>
      <td>77</td>
    </tr>
    <tr>
      <th>49</th>
      <td>PixelDefend: Leveraging Generative Models to Understand and Defend\n  against Adversarial Examples</td>
      <td>73</td>
    </tr>
    <tr>
      <th>50</th>
      <td>Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong</td>
      <td>67</td>
    </tr>
    <tr>
      <th>51</th>
      <td>Certifying Some Distributional Robustness with Principled Adversarial\n  Training</td>
      <td>67</td>
    </tr>
    <tr>
      <th>52</th>
      <td>NO Need to Worry about Adversarial Examples in Object Detection in\n  Autonomous Vehicles</td>
      <td>63</td>
    </tr>
    <tr>
      <th>53</th>
      <td>SafetyNet: Detecting and Rejecting Adversarial Examples Robustly</td>
      <td>61</td>
    </tr>
    <tr>
      <th>54</th>
      <td>Adversarial Examples: Attacks and Defenses for Deep Learning</td>
      <td>60</td>
    </tr>
    <tr>
      <th>55</th>
      <td>Adversarial Diversity and Hard Positive Generation</td>
      <td>56</td>
    </tr>
    <tr>
      <th>56</th>
      <td>Crafting Adversarial Input Sequences for Recurrent Neural Networks</td>
      <td>53</td>
    </tr>
    <tr>
      <th>57</th>
      <td>Characterizing Adversarial Subspaces Using Local Intrinsic\n  Dimensionality</td>
      <td>53</td>
    </tr>
    <tr>
      <th>58</th>
      <td>Houdini: Fooling Deep Structured Prediction Models</td>
      <td>53</td>
    </tr>
    <tr>
      <th>59</th>
      <td>Hacking Smart Machines with Smarter Ones: How to Extract Meaningful Data\n  from Machine Learnin...</td>
      <td>52</td>
    </tr>
    <tr>
      <th>60</th>
      <td>SalGAN: Visual Saliency Prediction with Generative Adversarial Networks</td>
      <td>51</td>
    </tr>
    <tr>
      <th>61</th>
      <td>BadNets: Identifying Vulnerabilities in the Machine Learning Model\n  Supply Chain</td>
      <td>50</td>
    </tr>
    <tr>
      <th>62</th>
      <td>Robust Adversarial Reinforcement Learning</td>
      <td>49</td>
    </tr>
    <tr>
      <th>63</th>
      <td>Stochastic Activation Pruning for Robust Adversarial Defense</td>
      <td>48</td>
    </tr>
    <tr>
      <th>64</th>
      <td>A study of the effect of JPG compression on adversarial images</td>
      <td>47</td>
    </tr>
    <tr>
      <th>65</th>
      <td>A Rotation and a Translation Suffice: Fooling CNNs with Simple\n  Transformations</td>
      <td>47</td>
    </tr>
    <tr>
      <th>66</th>
      <td>Efficient Defenses Against Adversarial Attacks</td>
      <td>46</td>
    </tr>
    <tr>
      <th>67</th>
      <td>Spatially Transformed Adversarial Examples</td>
      <td>46</td>
    </tr>
    <tr>
      <th>68</th>
      <td>Foolbox: A Python toolbox to benchmark the robustness of machine\n  learning models</td>
      <td>45</td>
    </tr>
    <tr>
      <th>69</th>
      <td>Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with\n  JPEG Compression</td>
      <td>44</td>
    </tr>
    <tr>
      <th>70</th>
      <td>Adversarial Transformation Networks: Learning to Generate Adversarial\n  Examples</td>
      <td>44</td>
    </tr>
    <tr>
      <th>71</th>
      <td>MagNet and "Efficient Defenses Against Adversarial Attacks" are Not\n  Robust to Adversarial Exa...</td>
      <td>43</td>
    </tr>
    <tr>
      <th>72</th>
      <td>Improving the Adversarial Robustness and Interpretability of Deep Neural\n  Networks by Regulari...</td>
      <td>42</td>
    </tr>
    <tr>
      <th>73</th>
      <td>LR-GAN: Layered Recursive Generative Adversarial Networks for Image\n  Generation</td>
      <td>42</td>
    </tr>
    <tr>
      <th>74</th>
      <td>A Boundary Tilting Persepective on the Phenomenon of Adversarial\n  Examples</td>
      <td>41</td>
    </tr>
    <tr>
      <th>75</th>
      <td>Tactics of Adversarial Attack on Deep Reinforcement Learning Agents</td>
      <td>38</td>
    </tr>
    <tr>
      <th>76</th>
      <td>Foveation-based Mechanisms Alleviate Adversarial Examples</td>
      <td>38</td>
    </tr>
    <tr>
      <th>77</th>
      <td>Early Methods for Detecting Adversarial Images</td>
      <td>38</td>
    </tr>
    <tr>
      <th>78</th>
      <td>Sensitivity and Generalization in Neural Networks: an Empirical Study</td>
      <td>37</td>
    </tr>
    <tr>
      <th>79</th>
      <td>Certified Defenses for Data Poisoning Attacks</td>
      <td>36</td>
    </tr>
    <tr>
      <th>80</th>
      <td>Universal Adversarial Perturbations Against Semantic Image Segmentation</td>
      <td>36</td>
    </tr>
    <tr>
      <th>81</th>
      <td>A Dual Approach to Scalable Verification of Deep Networks</td>
      <td>35</td>
    </tr>
    <tr>
      <th>82</th>
      <td>Adversarial Examples that Fool both Computer Vision and Time-Limited\n  Humans</td>
      <td>35</td>
    </tr>
    <tr>
      <th>83</th>
      <td>Generating Adversarial Examples with Adversarial Networks</td>
      <td>33</td>
    </tr>
    <tr>
      <th>84</th>
      <td>Evaluating the Robustness of Neural Networks: An Extreme Value Theory\n  Approach</td>
      <td>33</td>
    </tr>
    <tr>
      <th>85</th>
      <td>Machine Learning Models that Remember Too Much</td>
      <td>32</td>
    </tr>
    <tr>
      <th>86</th>
      <td>Fast Feature Fool: A data independent approach to universal adversarial\n  perturbations</td>
      <td>32</td>
    </tr>
    <tr>
      <th>87</th>
      <td>Black-box Adversarial Attacks with Limited Queries and Information</td>
      <td>32</td>
    </tr>
    <tr>
      <th>88</th>
      <td>Adversarial vulnerability for any classifier</td>
      <td>31</td>
    </tr>
    <tr>
      <th>89</th>
      <td>Adversarial Logit Pairing</td>
      <td>31</td>
    </tr>
    <tr>
      <th>90</th>
      <td>Robustness May Be at Odds with Accuracy</td>
      <td>29</td>
    </tr>
    <tr>
      <th>91</th>
      <td>Feature-Guided Black-Box Safety Testing of Deep Neural Networks</td>
      <td>28</td>
    </tr>
    <tr>
      <th>92</th>
      <td>Towards Proving the Adversarial Robustness of Deep Neural Networks</td>
      <td>27</td>
    </tr>
    <tr>
      <th>93</th>
      <td>Adversarial Spheres</td>
      <td>27</td>
    </tr>
    <tr>
      <th>94</th>
      <td>Delving into adversarial attacks on deep policies</td>
      <td>26</td>
    </tr>
    <tr>
      <th>95</th>
      <td>Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning</td>
      <td>26</td>
    </tr>
    <tr>
      <th>96</th>
      <td>Adversarial Attacks and Defences Competition</td>
      <td>26</td>
    </tr>
    <tr>
      <th>97</th>
      <td>Towards Crafting Text Adversarial Samples</td>
      <td>26</td>
    </tr>
    <tr>
      <th>98</th>
      <td>The Robust Manifold Defense: Adversarial Training using Generative\n  Models</td>
      <td>25</td>
    </tr>
    <tr>
      <th>99</th>
      <td>Adversarial Patch</td>
      <td>25</td>
    </tr>
    <tr>
      <th>100</th>
      <td>Exploring the Space of Black-box Attacks on Deep Neural Networks</td>
      <td>25</td>
    </tr>
    <tr>
      <th>101</th>
      <td>Adversarial Image Perturbation for Privacy Protection -- A Game Theory\n  Perspective</td>
      <td>24</td>
    </tr>
    <tr>
      <th>102</th>
      <td>Automatic Liver Segmentation Using an Adversarial Image-to-Image Network</td>
      <td>24</td>
    </tr>
    <tr>
      <th>103</th>
      <td>Generative Adversarial Trainer: Defense to Adversarial Perturbations\n  with GAN</td>
      <td>23</td>
    </tr>
    <tr>
      <th>104</th>
      <td>Dense Associative Memory is Robust to Adversarial Inputs</td>
      <td>23</td>
    </tr>
    <tr>
      <th>105</th>
      <td>Extending Defensive Distillation</td>
      <td>22</td>
    </tr>
    <tr>
      <th>106</th>
      <td>Analysis of universal adversarial perturbations</td>
      <td>22</td>
    </tr>
    <tr>
      <th>107</th>
      <td>Evaluating Robustness of Neural Networks with Mixed Integer Programming</td>
      <td>22</td>
    </tr>
    <tr>
      <th>108</th>
      <td>Robust Convolutional Neural Networks under Adversarial Noise</td>
      <td>22</td>
    </tr>
    <tr>
      <th>109</th>
      <td>Deflecting Adversarial Attacks with Pixel Deflection</td>
      <td>22</td>
    </tr>
    <tr>
      <th>110</th>
      <td>Towards Interpretable Deep Neural Networks by Leveraging Adversarial\n  Examples</td>
      <td>22</td>
    </tr>
    <tr>
      <th>111</th>
      <td>Motivating the Rules of the Game for Adversarial Example Research</td>
      <td>21</td>
    </tr>
    <tr>
      <th>112</th>
      <td>On the Robustness of the CVPR 2018 White-Box Adversarial Example\n  Defenses</td>
      <td>21</td>
    </tr>
    <tr>
      <th>113</th>
      <td>APE-GAN: Adversarial Perturbation Elimination with GAN</td>
      <td>21</td>
    </tr>
    <tr>
      <th>114</th>
      <td>Learn To Pay Attention</td>
      <td>21</td>
    </tr>
    <tr>
      <th>115</th>
      <td>Defense against Universal Adversarial Perturbations</td>
      <td>20</td>
    </tr>
    <tr>
      <th>116</th>
      <td>Cascade Adversarial Machine Learning Regularized with a Unified\n  Embedding</td>
      <td>20</td>
    </tr>
    <tr>
      <th>117</th>
      <td>Standard detectors aren't (currently) fooled by physical adversarial\n  stop signs</td>
      <td>20</td>
    </tr>
    <tr>
      <th>118</th>
      <td>Classification regions of deep neural networks</td>
      <td>19</td>
    </tr>
    <tr>
      <th>119</th>
      <td>Adversarial Examples that Fool Detectors</td>
      <td>18</td>
    </tr>
    <tr>
      <th>120</th>
      <td>Did you hear that? Adversarial Examples Against Automatic Speech\n  Recognition</td>
      <td>18</td>
    </tr>
    <tr>
      <th>121</th>
      <td>Robustness to Adversarial Examples through an Ensemble of Specialists</td>
      <td>18</td>
    </tr>
    <tr>
      <th>122</th>
      <td>Feature Squeezing Mitigates and Detects Carlini/Wagner Adversarial\n  Examples</td>
      <td>18</td>
    </tr>
    <tr>
      <th>123</th>
      <td>Testing Deep Neural Networks</td>
      <td>18</td>
    </tr>
    <tr>
      <th>124</th>
      <td>Adversarial Examples for Semantic Image Segmentation</td>
      <td>18</td>
    </tr>
    <tr>
      <th>125</th>
      <td>Understanding Measures of Uncertainty for Adversarial Example Detection</td>
      <td>17</td>
    </tr>
    <tr>
      <th>126</th>
      <td>On the Effectiveness of Defensive Distillation</td>
      <td>17</td>
    </tr>
    <tr>
      <th>127</th>
      <td>Generative Poisoning Attack Method Against Neural Networks</td>
      <td>17</td>
    </tr>
    <tr>
      <th>128</th>
      <td>ShapeShifter: Robust Physical Adversarial Attack on Faster R-CNN Object\n  Detector</td>
      <td>17</td>
    </tr>
    <tr>
      <th>129</th>
      <td>Ensemble Methods as a Defense to Adversarial Perturbations Against Deep\n  Neural Networks</td>
      <td>16</td>
    </tr>
    <tr>
      <th>130</th>
      <td>Detecting Adversarial Image Examples in Deep Networks with Adaptive\n  Noise Reduction</td>
      <td>16</td>
    </tr>
    <tr>
      <th>131</th>
      <td>Is Robustness the Cost of Accuracy? -- A Comprehensive Study on the\n  Robustness of 18 Deep Ima...</td>
      <td>16</td>
    </tr>
    <tr>
      <th>132</th>
      <td>CycleGAN, a Master of Steganography</td>
      <td>16</td>
    </tr>
    <tr>
      <th>133</th>
      <td>Are Facial Attributes Adversarially Robust?</td>
      <td>16</td>
    </tr>
    <tr>
      <th>134</th>
      <td>Detection of Unauthorized IoT Devices Using Machine Learning Techniques</td>
      <td>16</td>
    </tr>
    <tr>
      <th>135</th>
      <td>Lipschitz-Margin Training: Scalable Certification of Perturbation\n  Invariance for Deep Neural ...</td>
      <td>16</td>
    </tr>
    <tr>
      <th>136</th>
      <td>Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe\n  Noise</td>
      <td>16</td>
    </tr>
    <tr>
      <th>137</th>
      <td>Training verified learners with learned verifiers</td>
      <td>15</td>
    </tr>
    <tr>
      <th>138</th>
      <td>Unravelling Robustness of Deep Learning based Face Recognition Against\n  Adversarial Attacks</td>
      <td>15</td>
    </tr>
    <tr>
      <th>139</th>
      <td>Reinforcement Learning with a Corrupted Reward Channel</td>
      <td>15</td>
    </tr>
    <tr>
      <th>140</th>
      <td>How To Backdoor Federated Learning</td>
      <td>15</td>
    </tr>
    <tr>
      <th>141</th>
      <td>Intriguing Properties of Adversarial Examples</td>
      <td>15</td>
    </tr>
    <tr>
      <th>142</th>
      <td>A General Retraining Framework for Scalable Adversarial Classification</td>
      <td>14</td>
    </tr>
    <tr>
      <th>143</th>
      <td>Sanity Checks for Saliency Maps</td>
      <td>14</td>
    </tr>
    <tr>
      <th>144</th>
      <td>CommanderSong: A Systematic Approach for Practical Adversarial Voice\n  Recognition</td>
      <td>14</td>
    </tr>
    <tr>
      <th>145</th>
      <td>Shield: Fast, Practical Defense and Vaccination for Deep Learning using\n  JPEG Compression</td>
      <td>14</td>
    </tr>
    <tr>
      <th>146</th>
      <td>Towards Open-Set Identity Preserving Face Synthesis</td>
      <td>14</td>
    </tr>
    <tr>
      <th>147</th>
      <td>Adversarial Dropout for Supervised and Semi-supervised Learning</td>
      <td>14</td>
    </tr>
    <tr>
      <th>148</th>
      <td>Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks\n  by Backdooring</td>
      <td>13</td>
    </tr>
    <tr>
      <th>149</th>
      <td>AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for\n  Attacking Black-box Neural N...</td>
      <td>13</td>
    </tr>
    <tr>
      <th>150</th>
      <td>DARTS: Deceiving Autonomous Cars with Toxic Signs</td>
      <td>13</td>
    </tr>
    <tr>
      <th>151</th>
      <td>Are Accuracy and Robustness Correlated?</td>
      <td>13</td>
    </tr>
    <tr>
      <th>152</th>
      <td>Attacking Binarized Neural Networks</td>
      <td>13</td>
    </tr>
    <tr>
      <th>153</th>
      <td>Differentially Private Empirical Risk Minimization Revisited: Faster and\n  More General</td>
      <td>12</td>
    </tr>
    <tr>
      <th>154</th>
      <td>LaVAN: Localized and Visible Adversarial Noise</td>
      <td>12</td>
    </tr>
    <tr>
      <th>155</th>
      <td>Assessing Threat of Adversarial Examples on Deep Neural Networks</td>
      <td>12</td>
    </tr>
    <tr>
      <th>156</th>
      <td>Adversarial Images for Variational Autoencoders</td>
      <td>12</td>
    </tr>
    <tr>
      <th>157</th>
      <td>A General Framework for Adversarial Examples with Objectives</td>
      <td>12</td>
    </tr>
    <tr>
      <th>158</th>
      <td>Privacy Risk in Machine Learning: Analyzing the Connection to\n  Overfitting</td>
      <td>11</td>
    </tr>
    <tr>
      <th>159</th>
      <td>Adversarial Frontier Stitching for Remote Neural Network Watermarking</td>
      <td>11</td>
    </tr>
    <tr>
      <th>160</th>
      <td>Convolutional Networks with Adaptive Inference Graphs</td>
      <td>11</td>
    </tr>
    <tr>
      <th>161</th>
      <td>Towards Reverse-Engineering Black-Box Neural Networks</td>
      <td>11</td>
    </tr>
    <tr>
      <th>162</th>
      <td>Adversarial Vulnerability of Neural Networks Increases With Input\n  Dimension</td>
      <td>11</td>
    </tr>
    <tr>
      <th>163</th>
      <td>Stabilizing Adversarial Nets With Prediction Methods</td>
      <td>11</td>
    </tr>
    <tr>
      <th>164</th>
      <td>Adversarial Dropout Regularization</td>
      <td>11</td>
    </tr>
    <tr>
      <th>165</th>
      <td>Evaluating and Understanding the Robustness of Adversarial Logit Pairing</td>
      <td>10</td>
    </tr>
    <tr>
      <th>166</th>
      <td>Machine Learning with Membership Privacy using Adversarial\n  Regularization</td>
      <td>10</td>
    </tr>
    <tr>
      <th>167</th>
      <td>Ensemble Robustness and Generalization of Stochastic Deep Learning\n  Algorithms</td>
      <td>10</td>
    </tr>
    <tr>
      <th>168</th>
      <td>A Theoretical Framework for Robustness of (Deep) Classifiers against\n  Adversarial Examples</td>
      <td>10</td>
    </tr>
    <tr>
      <th>169</th>
      <td>Training for Faster Adversarial Robustness Verification via Inducing\n  ReLU Stability</td>
      <td>10</td>
    </tr>
    <tr>
      <th>170</th>
      <td>Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning</td>
      <td>9</td>
    </tr>
    <tr>
      <th>171</th>
      <td>LDMNet: Low Dimensional Manifold Regularized Neural Networks</td>
      <td>9</td>
    </tr>
    <tr>
      <th>172</th>
      <td>Generative Adversarial Active Learning</td>
      <td>9</td>
    </tr>
    <tr>
      <th>173</th>
      <td>Digital Watermarking for Deep Neural Networks</td>
      <td>9</td>
    </tr>
    <tr>
      <th>174</th>
      <td>Towards the first adversarially robust neural network model on MNIST</td>
      <td>9</td>
    </tr>
    <tr>
      <th>175</th>
      <td>Generative Adversarial Perturbations</td>
      <td>9</td>
    </tr>
    <tr>
      <th>176</th>
      <td>Attacking Visual Language Grounding with Adversarial Examples: A Case\n  Study on Neural Image C...</td>
      <td>9</td>
    </tr>
    <tr>
      <th>177</th>
      <td>Deep Co-Training for Semi-Supervised Image Recognition</td>
      <td>9</td>
    </tr>
    <tr>
      <th>178</th>
      <td>Adversarial Vision Challenge</td>
      <td>9</td>
    </tr>
    <tr>
      <th>179</th>
      <td>Slalom: Fast, Verifiable and Private Execution of Neural Networks in\n  Trusted Hardware</td>
      <td>9</td>
    </tr>
    <tr>
      <th>180</th>
      <td>Targeted Adversarial Examples for Black Box Audio Systems</td>
      <td>9</td>
    </tr>
    <tr>
      <th>181</th>
      <td>Constructing Unrestricted Adversarial Examples with Generative Models</td>
      <td>8</td>
    </tr>
    <tr>
      <th>182</th>
      <td>Towards Imperceptible and Robust Adversarial Example Attacks against\n  Neural Networks</td>
      <td>8</td>
    </tr>
    <tr>
      <th>183</th>
      <td>Randomized Prediction Games for Adversarial Machine Learning</td>
      <td>8</td>
    </tr>
    <tr>
      <th>184</th>
      <td>Certified Robustness to Adversarial Examples with Differential Privacy</td>
      <td>8</td>
    </tr>
    <tr>
      <th>185</th>
      <td>Rogue Signs: Deceiving Traffic Sign Recognition with Malicious Ads and\n  Logos</td>
      <td>8</td>
    </tr>
    <tr>
      <th>186</th>
      <td>Detecting Adversarial Attacks on Neural Network Policies with Visual\n  Foresight</td>
      <td>8</td>
    </tr>
    <tr>
      <th>187</th>
      <td>MTDeep: Boosting the Security of Deep Neural Nets Against Adversarial\n  Attacks with Moving Tar...</td>
      <td>8</td>
    </tr>
    <tr>
      <th>188</th>
      <td>Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural\n  Networks</td>
      <td>8</td>
    </tr>
    <tr>
      <th>189</th>
      <td>Are adversarial examples inevitable?</td>
      <td>8</td>
    </tr>
    <tr>
      <th>190</th>
      <td>Adversarial Attacks Beyond the Image Space</td>
      <td>8</td>
    </tr>
    <tr>
      <th>191</th>
      <td>Physical Adversarial Examples for Object Detectors</td>
      <td>8</td>
    </tr>
    <tr>
      <th>192</th>
      <td>Note on Attacking Object Detectors with Adversarial Stickers</td>
      <td>8</td>
    </tr>
    <tr>
      <th>193</th>
      <td>Regularizing deep networks using efficient layerwise adversarial\n  training</td>
      <td>8</td>
    </tr>
    <tr>
      <th>194</th>
      <td>Suppressing the Unusual: towards Robust CNNs using Symmetric Activation\n  Functions</td>
      <td>8</td>
    </tr>
    <tr>
      <th>195</th>
      <td>Unrestricted Adversarial Examples</td>
      <td>7</td>
    </tr>
    <tr>
      <th>196</th>
      <td>On the Effectiveness of Interval Bound Propagation for Training\n  Verifiably Robust Models</td>
      <td>7</td>
    </tr>
    <tr>
      <th>197</th>
      <td>Fortified Networks: Improving the Robustness of Deep Networks by\n  Modeling the Manifold of Hid...</td>
      <td>7</td>
    </tr>
    <tr>
      <th>198</th>
      <td>Summoning Demons: The Pursuit of Exploitable Bugs in Machine Learning</td>
      <td>7</td>
    </tr>
    <tr>
      <th>199</th>
      <td>Adversarial Active Learning for Deep Networks: a Margin Based Approach</td>
      <td>7</td>
    </tr>
    <tr>
      <th>200</th>
      <td>Defending Against Adversarial Attacks by Leveraging an Entire GAN</td>
      <td>7</td>
    </tr>
    <tr>
      <th>201</th>
      <td>GenAttack: Practical Black-box Attacks with Gradient-Free Optimization</td>
      <td>7</td>
    </tr>
    <tr>
      <th>202</th>
      <td>On the Suitability of $L_p$-norms for Creating and Preventing\n  Adversarial Examples</td>
      <td>7</td>
    </tr>
    <tr>
      <th>203</th>
      <td>Adversarial Attacks on Face Detectors using Neural Net based Constrained\n  Optimization</td>
      <td>7</td>
    </tr>
    <tr>
      <th>204</th>
      <td>Facial Attributes: Accuracy and Adversarial Robustness</td>
      <td>7</td>
    </tr>
    <tr>
      <th>205</th>
      <td>On the Limitation of Convolutional Neural Networks in Recognizing\n  Negative Images</td>
      <td>7</td>
    </tr>
    <tr>
      <th>206</th>
      <td>Generalizable Data-free Objective for Crafting Universal Adversarial\n  Perturbations</td>
      <td>6</td>
    </tr>
    <tr>
      <th>207</th>
      <td>Detecting Backdoor Attacks on Deep Neural Networks by Activation\n  Clustering</td>
      <td>6</td>
    </tr>
    <tr>
      <th>208</th>
      <td>HiDDeN: Hiding Data With Deep Networks</td>
      <td>6</td>
    </tr>
    <tr>
      <th>209</th>
      <td>A Counter-Forensic Method for CNN-Based Camera Model Identification</td>
      <td>6</td>
    </tr>
    <tr>
      <th>210</th>
      <td>Manifold Regularized Deep Neural Networks using Adversarial Examples</td>
      <td>6</td>
    </tr>
    <tr>
      <th>211</th>
      <td>Novel Deep Learning Model for Traffic Sign Detection Using Capsule\n  Networks</td>
      <td>6</td>
    </tr>
    <tr>
      <th>212</th>
      <td>UPSET and ANGRI : Breaking High Performance Image Classifiers</td>
      <td>6</td>
    </tr>
    <tr>
      <th>213</th>
      <td>MAGIX: Model Agnostic Globally Interpretable Explanations</td>
      <td>6</td>
    </tr>
    <tr>
      <th>214</th>
      <td>Art of singular vectors and universal adversarial perturbations</td>
      <td>5</td>
    </tr>
    <tr>
      <th>215</th>
      <td>Training Ensembles to Detect Adversarial Examples</td>
      <td>5</td>
    </tr>
    <tr>
      <th>216</th>
      <td>Adversarial Examples Detection in Deep Networks with Convolutional\n  Filter Statistics</td>
      <td>5</td>
    </tr>
    <tr>
      <th>217</th>
      <td>Label Sanitization against Label Flipping Poisoning Attacks</td>
      <td>5</td>
    </tr>
    <tr>
      <th>218</th>
      <td>Defend Deep Neural Networks Against Adversarial Examples via Fixed\n  andDynamic Quantized Activ...</td>
      <td>5</td>
    </tr>
    <tr>
      <th>219</th>
      <td>Improving Transferability of Adversarial Examples with Input Diversity</td>
      <td>5</td>
    </tr>
    <tr>
      <th>220</th>
      <td>Robustness of Rotation-Equivariant Networks to Adversarial Perturbations</td>
      <td>5</td>
    </tr>
    <tr>
      <th>221</th>
      <td>On the Limitation of Local Intrinsic Dimensionality for Characterizing\n  the Subspaces of Adver...</td>
      <td>5</td>
    </tr>
    <tr>
      <th>222</th>
      <td>Semantic Adversarial Examples</td>
      <td>5</td>
    </tr>
    <tr>
      <th>223</th>
      <td>Divide, Denoise, and Defend against Adversarial Attacks</td>
      <td>5</td>
    </tr>
    <tr>
      <th>224</th>
      <td>Feature Denoising for Improving Adversarial Robustness</td>
      <td>5</td>
    </tr>
    <tr>
      <th>225</th>
      <td>Improving DNN Robustness to Adversarial Attacks using Jacobian\n  Regularization</td>
      <td>5</td>
    </tr>
    <tr>
      <th>226</th>
      <td>Benchmarking Neural Network Robustness to Common Corruptions and\n  Perturbations</td>
      <td>5</td>
    </tr>
    <tr>
      <th>227</th>
      <td>Adversarial Attacks and Defences: A Survey</td>
      <td>4</td>
    </tr>
    <tr>
      <th>228</th>
      <td>Security and Privacy Issues in Deep Learning</td>
      <td>4</td>
    </tr>
    <tr>
      <th>229</th>
      <td>Securing Distributed Machine Learning in High Dimensions</td>
      <td>4</td>
    </tr>
    <tr>
      <th>230</th>
      <td>A Game-Theoretic Analysis of Adversarial Classification</td>
      <td>4</td>
    </tr>
    <tr>
      <th>231</th>
      <td>Hu-Fu: Hardware and Software Collaborative Attack Framework against\n  Neural Networks</td>
      <td>4</td>
    </tr>
    <tr>
      <th>232</th>
      <td>Robust Machine Comprehension Models via Adversarial Training</td>
      <td>4</td>
    </tr>
    <tr>
      <th>233</th>
      <td>Gradient Masking Causes CLEVER to Overestimate Adversarial Perturbation\n  Size</td>
      <td>4</td>
    </tr>
    <tr>
      <th>234</th>
      <td>Spectral Signatures in Backdoor Attacks</td>
      <td>4</td>
    </tr>
    <tr>
      <th>235</th>
      <td>Does Your Model Know the Digit 6 Is Not a Cat? A Less Biased Evaluation\n  of "Outlier" Detectors</td>
      <td>4</td>
    </tr>
    <tr>
      <th>236</th>
      <td>NAG: Network for Adversary Generation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>237</th>
      <td>Improving the Generalization of Adversarial Training with Domain\n  Adaptation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>238</th>
      <td>Global Robustness Evaluation of Deep Neural Networks with Provable\n  Guarantees for the $L_0$ Norm</td>
      <td>4</td>
    </tr>
    <tr>
      <th>239</th>
      <td>Sparse DNNs with Improved Adversarial Robustness</td>
      <td>4</td>
    </tr>
    <tr>
      <th>240</th>
      <td>ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and\n  Uncovering Biases</td>
      <td>4</td>
    </tr>
    <tr>
      <th>241</th>
      <td>Lightweight Probabilistic Deep Networks</td>
      <td>4</td>
    </tr>
    <tr>
      <th>242</th>
      <td>Feature Distillation: DNN-Oriented JPEG Compression Against Adversarial\n  Examples</td>
      <td>4</td>
    </tr>
    <tr>
      <th>243</th>
      <td>Unsupervised Histopathology Image Synthesis</td>
      <td>4</td>
    </tr>
    <tr>
      <th>244</th>
      <td>Beyond Pixel Norm-Balls: Parametric Adversaries using an Analytically\n  Differentiable Renderer</td>
      <td>4</td>
    </tr>
    <tr>
      <th>245</th>
      <td>Interpretable Convolutional Neural Networks via Feedforward Design</td>
      <td>4</td>
    </tr>
    <tr>
      <th>246</th>
      <td>Learning Visually-Grounded Semantics from Contrastive Adversarial\n  Samples</td>
      <td>4</td>
    </tr>
    <tr>
      <th>247</th>
      <td>PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks</td>
      <td>4</td>
    </tr>
    <tr>
      <th>248</th>
      <td>HyperNetworks with statistical filtering for defending adversarial\n  examples</td>
      <td>4</td>
    </tr>
    <tr>
      <th>249</th>
      <td>Hierarchical interpretations for neural network predictions</td>
      <td>4</td>
    </tr>
    <tr>
      <th>250</th>
      <td>Low Frequency Adversarial Perturbation</td>
      <td>4</td>
    </tr>
    <tr>
      <th>251</th>
      <td>Attacking Convolutional Neural Network using Differential Evolution</td>
      <td>4</td>
    </tr>
    <tr>
      <th>252</th>
      <td>Sparse Adversarial Perturbations for Videos</td>
      <td>4</td>
    </tr>
    <tr>
      <th>253</th>
      <td>Generative Adversarial Image Synthesis with Decision Tree Latent\n  Controller</td>
      <td>4</td>
    </tr>
    <tr>
      <th>254</th>
      <td>Adversarially Robust Training through Structured Gradient Regularization</td>
      <td>4</td>
    </tr>
    <tr>
      <th>255</th>
      <td>Adversarial Attacks on Node Embeddings</td>
      <td>4</td>
    </tr>
    <tr>
      <th>256</th>
      <td>Defense Against the Dark Arts: An overview of adversarial example\n  security research and futur...</td>
      <td>3</td>
    </tr>
    <tr>
      <th>257</th>
      <td>Limitations of the Lipschitz constant as a defense against adversarial\n  examples</td>
      <td>3</td>
    </tr>
    <tr>
      <th>258</th>
      <td>Playing the Game of Universal Adversarial Perturbations</td>
      <td>3</td>
    </tr>
    <tr>
      <th>259</th>
      <td>Detecting Adversarial Perturbations with Saliency</td>
      <td>3</td>
    </tr>
    <tr>
      <th>260</th>
      <td>Hardware Trojan Attacks on Neural Networks</td>
      <td>3</td>
    </tr>
    <tr>
      <th>261</th>
      <td>Attack Strength vs. Detectability Dilemma in Adversarial Machine\n  Learning</td>
      <td>3</td>
    </tr>
    <tr>
      <th>262</th>
      <td>Resisting Adversarial Attacks using Gaussian Mixture Variational\n  Autoencoders</td>
      <td>3</td>
    </tr>
    <tr>
      <th>263</th>
      <td>Disentangling Adversarial Robustness and Generalization</td>
      <td>3</td>
    </tr>
    <tr>
      <th>264</th>
      <td>Defense Against Adversarial Attacks with Saak Transform</td>
      <td>3</td>
    </tr>
    <tr>
      <th>265</th>
      <td>Robust Classification with Convolutional Prototype Learning</td>
      <td>3</td>
    </tr>
    <tr>
      <th>266</th>
      <td>VectorDefense: Vectorization as a Defense to Adversarial Examples</td>
      <td>3</td>
    </tr>
    <tr>
      <th>267</th>
      <td>Adversarial Robustness: Softmax versus Openmax</td>
      <td>3</td>
    </tr>
    <tr>
      <th>268</th>
      <td>A Novel Framework for Robustness Analysis of Visual QA Models</td>
      <td>3</td>
    </tr>
    <tr>
      <th>269</th>
      <td>Universal Adversarial Training</td>
      <td>3</td>
    </tr>
    <tr>
      <th>270</th>
      <td>Adversarial Defense based on Structure-to-Signal Autoencoders</td>
      <td>3</td>
    </tr>
    <tr>
      <th>271</th>
      <td>On the Sensitivity of Adversarial Robustness to Input Data Distributions</td>
      <td>3</td>
    </tr>
    <tr>
      <th>272</th>
      <td>Anonymizing k-Facial Attributes via Adversarial Perturbations</td>
      <td>3</td>
    </tr>
    <tr>
      <th>273</th>
      <td>Deep Dictionary Learning: A PARametric NETwork Approach</td>
      <td>3</td>
    </tr>
    <tr>
      <th>274</th>
      <td>Adversarial Image Registration with Application for MR and TRUS Image\n  Fusion</td>
      <td>3</td>
    </tr>
    <tr>
      <th>275</th>
      <td>Combinatorial Attacks on Binarized Neural Networks</td>
      <td>3</td>
    </tr>
    <tr>
      <th>276</th>
      <td>Parametric Adversarial Divergences are Good Task Losses for Generative\n  Modeling</td>
      <td>3</td>
    </tr>
    <tr>
      <th>277</th>
      <td>Defensive Quantization: When Efficiency Meets Robustness</td>
      <td>3</td>
    </tr>
    <tr>
      <th>278</th>
      <td>Realistic Adversarial Examples in 3D Meshes</td>
      <td>2</td>
    </tr>
    <tr>
      <th>279</th>
      <td>On Evaluating Adversarial Robustness</td>
      <td>2</td>
    </tr>
    <tr>
      <th>280</th>
      <td>Security Risks in Deep Learning Implementations</td>
      <td>2</td>
    </tr>
    <tr>
      <th>281</th>
      <td>Adversarial Attacks Against Automatic Speech Recognition Systems via\n  Psychoacoustic Hiding</td>
      <td>2</td>
    </tr>
    <tr>
      <th>282</th>
      <td>An Overview of Vulnerabilities of Voice Controlled Systems</td>
      <td>2</td>
    </tr>
    <tr>
      <th>283</th>
      <td>DARCCC: Detecting Adversaries by Reconstruction from Class Conditional\n  Capsules</td>
      <td>2</td>
    </tr>
    <tr>
      <th>284</th>
      <td>Isolated and Ensemble Audio Preprocessing Methods for Detecting\n  Adversarial Examples against ...</td>
      <td>2</td>
    </tr>
    <tr>
      <th>285</th>
      <td>Adversarial Reprogramming of Neural Networks</td>
      <td>2</td>
    </tr>
    <tr>
      <th>286</th>
      <td>DeepMarks: A Digital Fingerprinting Framework for Deep Neural Networks</td>
      <td>2</td>
    </tr>
    <tr>
      <th>287</th>
      <td>Adaptive Adversarial Attack on Scene Text Recognition</td>
      <td>2</td>
    </tr>
    <tr>
      <th>288</th>
      <td>Adversarial Examples Are a Natural Consequence of Test Error in Noise</td>
      <td>2</td>
    </tr>
    <tr>
      <th>289</th>
      <td>The Limitations of Adversarial Training and the Blind-Spot Attack</td>
      <td>2</td>
    </tr>
    <tr>
      <th>290</th>
      <td>Deep Defense: Training DNNs with Improved Adversarial Robustness</td>
      <td>2</td>
    </tr>
    <tr>
      <th>291</th>
      <td>Generating 3D Adversarial Point Clouds</td>
      <td>2</td>
    </tr>
    <tr>
      <th>292</th>
      <td>Parametric Noise Injection: Trainable Randomness to Improve Deep Neural\n  Network Robustness ag...</td>
      <td>2</td>
    </tr>
    <tr>
      <th>293</th>
      <td>Adversarial Noise Layer: Regularize Neural Network By Adding Noise</td>
      <td>2</td>
    </tr>
    <tr>
      <th>294</th>
      <td>Characterizing Adversarial Examples Based on Spatial Consistency\n  Information for Semantic Seg...</td>
      <td>2</td>
    </tr>
    <tr>
      <th>295</th>
      <td>Adversarial Perturbations Against Real-Time Video Classification Systems</td>
      <td>2</td>
    </tr>
    <tr>
      <th>296</th>
      <td>Gradient Adversarial Training of Neural Networks</td>
      <td>2</td>
    </tr>
    <tr>
      <th>297</th>
      <td>LOTS about Attacking Deep Features</td>
      <td>2</td>
    </tr>
    <tr>
      <th>298</th>
      <td>A Learning and Masking Approach to Secure Learning</td>
      <td>2</td>
    </tr>
    <tr>
      <th>299</th>
      <td>QBDC: Query by dropout committee for training deep supervised\n  architecture</td>
      <td>2</td>
    </tr>
    <tr>
      <th>300</th>
      <td>ADef: an Iterative Algorithm to Construct Adversarial Deformations</td>
      <td>2</td>
    </tr>
    <tr>
      <th>301</th>
      <td>Knowledge Distillation with Adversarial Samples Supporting Decision\n  Boundary</td>
      <td>2</td>
    </tr>
    <tr>
      <th>302</th>
      <td>Featurized Bidirectional GAN: Adversarial Defense via Adversarially\n  Learned Semantic Inference</td>
      <td>2</td>
    </tr>
    <tr>
      <th>303</th>
      <td>Robust Adversarial Perturbation on Deep Proposal-based Models</td>
      <td>2</td>
    </tr>
    <tr>
      <th>304</th>
      <td>Sequential Attacks on Agents for Long-Term Adversarial Goals</td>
      <td>2</td>
    </tr>
    <tr>
      <th>305</th>
      <td>Targeted Nonlinear Adversarial Perturbations in Images and Videos</td>
      <td>2</td>
    </tr>
    <tr>
      <th>306</th>
      <td>Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box\n  Machine Learning Models</td>
      <td>2</td>
    </tr>
    <tr>
      <th>307</th>
      <td>Wolf in Sheep's Clothing - The Downscaling Attack Against Deep Learning\n  Applications</td>
      <td>2</td>
    </tr>
    <tr>
      <th>308</th>
      <td>Fooling OCR Systems with Adversarial Text Images</td>
      <td>2</td>
    </tr>
    <tr>
      <th>309</th>
      <td>AOGNets: Compositional Grammatical Architectures for Deep Learning</td>
      <td>2</td>
    </tr>
    <tr>
      <th>310</th>
      <td>Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural\n  Network</td>
      <td>2</td>
    </tr>
    <tr>
      <th>311</th>
      <td>Generalizable Adversarial Training via Spectral Normalization</td>
      <td>2</td>
    </tr>
    <tr>
      <th>312</th>
      <td>Anomaly Detection with Generative Adversarial Networks for Multivariate\n  Time Series</td>
      <td>2</td>
    </tr>
    <tr>
      <th>313</th>
      <td>Generalizability vs. Robustness: Adversarial Examples for Medical\n  Imaging</td>
      <td>1</td>
    </tr>
    <tr>
      <th>314</th>
      <td>Safely Entering the Deep: A Review of Verification and Validation for\n  Machine Learning and a ...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>315</th>
      <td>Less is More: Culling the Training Set to Improve Robustness of Deep\n  Neural Networks</td>
      <td>1</td>
    </tr>
    <tr>
      <th>316</th>
      <td>Learning Universal Adversarial Perturbations with Generative Models</td>
      <td>1</td>
    </tr>
    <tr>
      <th>317</th>
      <td>Bayesian Adversarial Spheres: Bayesian Inference and Adversarial\n  Examples in a Noiseless Setting</td>
      <td>1</td>
    </tr>
    <tr>
      <th>318</th>
      <td>High Dimensional Spaces, Deep Learning and Adversarial Examples</td>
      <td>1</td>
    </tr>
    <tr>
      <th>319</th>
      <td>SentiNet: Detecting Physical Attacks Against Deep Learning Systems</td>
      <td>1</td>
    </tr>
    <tr>
      <th>320</th>
      <td>A Method for Restoring the Training Set Distribution in an Image\n  Classifier</td>
      <td>1</td>
    </tr>
    <tr>
      <th>321</th>
      <td>All You Need is "Love": Evading Hate-speech Detection</td>
      <td>1</td>
    </tr>
    <tr>
      <th>322</th>
      <td>Generative Adversarial Active Learning for Unsupervised Outlier\n  Detection</td>
      <td>1</td>
    </tr>
    <tr>
      <th>323</th>
      <td>Adversarial Metric Learning</td>
      <td>1</td>
    </tr>
    <tr>
      <th>324</th>
      <td>Enhanced Attacks on Defensively Distilled Deep Neural Networks</td>
      <td>1</td>
    </tr>
    <tr>
      <th>325</th>
      <td>Differentially Private Empirical Risk Minimization with Input\n  Perturbation</td>
      <td>1</td>
    </tr>
    <tr>
      <th>326</th>
      <td>A Survey on Resilient Machine Learning</td>
      <td>1</td>
    </tr>
    <tr>
      <th>327</th>
      <td>Backdoor Embedding in Convolutional Neural Network Models via Invisible\n  Perturbation</td>
      <td>1</td>
    </tr>
    <tr>
      <th>328</th>
      <td>Backdooring Convolutional Neural Networks via Targeted Weight\n  Perturbations</td>
      <td>1</td>
    </tr>
    <tr>
      <th>329</th>
      <td>BEBP: An Poisoning Method Against Machine Learning Based IDSs</td>
      <td>1</td>
    </tr>
    <tr>
      <th>330</th>
      <td>Defending against Universal Perturbations with Shared Adversarial\n  Training</td>
      <td>1</td>
    </tr>
    <tr>
      <th>331</th>
      <td>Data Poisoning Attacks in Contextual Bandits</td>
      <td>1</td>
    </tr>
    <tr>
      <th>332</th>
      <td>Learning Transferable Adversarial Examples via Ghost Networks</td>
      <td>1</td>
    </tr>
    <tr>
      <th>333</th>
      <td>Excessive Invariance Causes Adversarial Vulnerability</td>
      <td>1</td>
    </tr>
    <tr>
      <th>334</th>
      <td>Adversarial Examples - A Complete Characterisation of the Phenomenon</td>
      <td>1</td>
    </tr>
    <tr>
      <th>335</th>
      <td>Towards an Understanding of Neural Networks in Natural-Image Spaces</td>
      <td>1</td>
    </tr>
    <tr>
      <th>336</th>
      <td>Guessing Smart: Biased Sampling for Efficient Black-Box Adversarial\n  Attacks</td>
      <td>1</td>
    </tr>
    <tr>
      <th>337</th>
      <td>The Efficacy of SHIELD under Different Threat Models</td>
      <td>1</td>
    </tr>
    <tr>
      <th>338</th>
      <td>Deflecting 3D Adversarial Point Clouds Through Outlier-Guided Removal</td>
      <td>1</td>
    </tr>
    <tr>
      <th>339</th>
      <td>Detecting Adversarial Perturbations Through Spatial Behavior in\n  Activation Spaces</td>
      <td>1</td>
    </tr>
    <tr>
      <th>340</th>
      <td>Reversible Adversarial Examples</td>
      <td>1</td>
    </tr>
    <tr>
      <th>341</th>
      <td>Decoupling Direction and Norm for Efficient Gradient-Based L2\n  Adversarial Attacks and Defenses</td>
      <td>1</td>
    </tr>
    <tr>
      <th>342</th>
      <td>Siamese Generative Adversarial Privatizer for Biometric Data</td>
      <td>1</td>
    </tr>
    <tr>
      <th>343</th>
      <td>Towards Query Efficient Black-box Attacks: An Input-free Perspective</td>
      <td>1</td>
    </tr>
    <tr>
      <th>344</th>
      <td>Open Set Adversarial Examples</td>
      <td>1</td>
    </tr>
    <tr>
      <th>345</th>
      <td>PathGAN: Visual Scanpath Prediction with Generative Adversarial Networks</td>
      <td>1</td>
    </tr>
    <tr>
      <th>346</th>
      <td>Traits &amp; Transferability of Adversarial Examples against Instance\n  Segmentation &amp; Object Detec...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>347</th>
      <td>Detecting Adversarial Examples via Key-based Network</td>
      <td>1</td>
    </tr>
    <tr>
      <th>348</th>
      <td>An ADMM-Based Universal Framework for Adversarial Attacks on Deep Neural\n  Networks</td>
      <td>1</td>
    </tr>
    <tr>
      <th>349</th>
      <td>Complement Objective Training</td>
      <td>1</td>
    </tr>
    <tr>
      <th>350</th>
      <td>A Convex Relaxation Barrier to Tight Robustness Verification of Neural\n  Networks</td>
      <td>1</td>
    </tr>
    <tr>
      <th>351</th>
      <td>Stroke-based Character Reconstruction</td>
      <td>1</td>
    </tr>
    <tr>
      <th>352</th>
      <td>A Simple Cache Model for Image Recognition</td>
      <td>1</td>
    </tr>
    <tr>
      <th>353</th>
      <td>Harmonic Adversarial Attack Method</td>
      <td>1</td>
    </tr>
    <tr>
      <th>354</th>
      <td>Auto-Context R-CNN</td>
      <td>1</td>
    </tr>
    <tr>
      <th>355</th>
      <td>Gradient Similarity: An Explainable Approach to Detect Adversarial\n  Attacks against Deep Learning</td>
      <td>1</td>
    </tr>
    <tr>
      <th>356</th>
      <td>Image Super-Resolution as a Defense Against Adversarial Attacks</td>
      <td>1</td>
    </tr>
    <tr>
      <th>357</th>
      <td>Robustness via curvature regularization, and vice versa</td>
      <td>1</td>
    </tr>
    <tr>
      <th>358</th>
      <td>Learning Discriminative Video Representations Using Adversarial\n  Perturbations</td>
      <td>1</td>
    </tr>
    <tr>
      <th>359</th>
      <td>Built-in Vulnerabilities to Imperceptible Adversarial Perturbations</td>
      <td>1</td>
    </tr>
    <tr>
      <th>360</th>
      <td>ADA: A Game-Theoretic Perspective on Data Augmentation for Object\n  Detection</td>
      <td>1</td>
    </tr>
    <tr>
      <th>361</th>
      <td>SPIGAN: Privileged Adversarial Learning from Simulation</td>
      <td>1</td>
    </tr>
    <tr>
      <th>362</th>
      <td>Implicit Generation and Generalization in Energy-Based Models</td>
      <td>1</td>
    </tr>
    <tr>
      <th>363</th>
      <td>Improved robustness to adversarial examples using Lipschitz\n  regularization of the loss</td>
      <td>1</td>
    </tr>
    <tr>
      <th>364</th>
      <td>Unsupervised Depth Estimation, 3D Face Rotation and Replacement</td>
      <td>1</td>
    </tr>
    <tr>
      <th>365</th>
      <td>GazeGAN - Unpaired Adversarial Image Generation for Gaze Estimation</td>
      <td>1</td>
    </tr>
    <tr>
      <th>366</th>
      <td>Grad-CAM: Visual Explanations from Deep Networks via Gradient-based\n  Localization</td>
      <td>1</td>
    </tr>
    <tr>
      <th>367</th>
      <td>Injecting and removing malignant features in mammography with CycleGAN:\n  Investigation of an a...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>368</th>
      <td>Accurate and Robust Neural Networks for Security Related Applications\n  Exampled by Face Morphi...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>369</th>
      <td>Structured Adversarial Attack: Towards General Implementation and Better\n  Interpretability</td>
      <td>1</td>
    </tr>
    <tr>
      <th>370</th>
      <td>Adversarial Machine Learning And Speech Emotion Recognition: Utilizing\n  Generative Adversarial...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>371</th>
      <td>Stable Distribution Alignment Using the Dual of the Adversarial Distance</td>
      <td>1</td>
    </tr>
    <tr>
      <th>372</th>
      <td>Adversarial Defense via Data Dependent Activation Function and Total\n  Variation Minimization</td>
      <td>1</td>
    </tr>
    <tr>
      <th>373</th>
      <td>Collaborative Sampling in Generative Adversarial Networks</td>
      <td>1</td>
    </tr>
    <tr>
      <th>374</th>
      <td>Adversarial Training for Free!</td>
      <td>1</td>
    </tr>
    <tr>
      <th>375</th>
      <td>Seeing isn't Believing: Practical Adversarial Attack Against Object\n  Detectors</td>
      <td>0</td>
    </tr>
    <tr>
      <th>376</th>
      <td>Practical Distributed Learning: Secure Machine Learning with\n  Communication-Efficient Local Up...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>377</th>
      <td>Discretization based Solutions for Secure Machine Learning against\n  Adversarial Attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>378</th>
      <td>New CleverHans Feature: Better Adversarial Robustness Evaluations with\n  Attack Bundling</td>
      <td>0</td>
    </tr>
    <tr>
      <th>379</th>
      <td>A Dynamic-Adversarial Mining Approach to the Security of Machine\n  Learning</td>
      <td>0</td>
    </tr>
    <tr>
      <th>380</th>
      <td>A Fundamental Performance Limitation for Adversarial Classification</td>
      <td>0</td>
    </tr>
    <tr>
      <th>381</th>
      <td>Adversarial classification: An adversarial risk analysis approach</td>
      <td>0</td>
    </tr>
    <tr>
      <th>382</th>
      <td>Adversarial Classification on Social Networks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>383</th>
      <td>Feature prioritization and regularization improve standard accuracy and\n  adversarial robustness</td>
      <td>0</td>
    </tr>
    <tr>
      <th>384</th>
      <td>Towards Robust Deep Neural Networks with BANG</td>
      <td>0</td>
    </tr>
    <tr>
      <th>385</th>
      <td>Lightweight Lipschitz Margin Training for Certified Defense against\n  Adversarial Examples</td>
      <td>0</td>
    </tr>
    <tr>
      <th>386</th>
      <td>Technical Report: When Does Machine Learning FAIL? Generalized\n  Transferability for Evasion an...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>387</th>
      <td>DPatch: An Adversarial Patch Attack on Object Detectors</td>
      <td>0</td>
    </tr>
    <tr>
      <th>388</th>
      <td>PRADA: Protecting against DNN Model Stealing Attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>389</th>
      <td>Training Set Camouflage</td>
      <td>0</td>
    </tr>
    <tr>
      <th>390</th>
      <td>Recognizing Disguised Faces in the Wild</td>
      <td>0</td>
    </tr>
    <tr>
      <th>391</th>
      <td>Generating Natural Adversarial Examples</td>
      <td>0</td>
    </tr>
    <tr>
      <th>392</th>
      <td>Adversarial Attacks Against Medical Deep Learning Systems</td>
      <td>0</td>
    </tr>
    <tr>
      <th>393</th>
      <td>Adversarial examples for generative models</td>
      <td>0</td>
    </tr>
    <tr>
      <th>394</th>
      <td>ReabsNet: Detecting and Revising Adversarial Examples</td>
      <td>0</td>
    </tr>
    <tr>
      <th>395</th>
      <td>Detecting Adversarial Examples via Neural Fingerprinting</td>
      <td>0</td>
    </tr>
    <tr>
      <th>396</th>
      <td>Bridging machine learning and cryptography in defence against\n  adversarial attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>397</th>
      <td>Energy Confused Adversarial Metric Learning for Zero-Shot Image\n  Retrieval and Clustering</td>
      <td>0</td>
    </tr>
    <tr>
      <th>398</th>
      <td>Extended Abstract: Mimicry Resilient Program Behavior Modeling with LSTM\n  based Branch Models</td>
      <td>0</td>
    </tr>
    <tr>
      <th>399</th>
      <td>Distributed Statistical Machine Learning in Adversarial Settings:\n  Byzantine Gradient Descent</td>
      <td>0</td>
    </tr>
    <tr>
      <th>400</th>
      <td>Analyzing and Improving Representations with the Soft Nearest Neighbor\n  Loss</td>
      <td>0</td>
    </tr>
    <tr>
      <th>401</th>
      <td>Universal Perturbation Attack Against Image Retrieval</td>
      <td>0</td>
    </tr>
    <tr>
      <th>402</th>
      <td>Have You Stolen My Model? Evasion Attacks Against Deep Neural Network\n  Watermarking Techniques</td>
      <td>0</td>
    </tr>
    <tr>
      <th>403</th>
      <td>Structure-Preserving Transformation: Generating Diverse and Transferable\n  Adversarial Examples</td>
      <td>0</td>
    </tr>
    <tr>
      <th>404</th>
      <td>CAAD 2018: Generating Transferable Adversarial Examples</td>
      <td>0</td>
    </tr>
    <tr>
      <th>405</th>
      <td>Exploiting Excessive Invariance caused by Norm-Bounded Adversarial\n  Robustness</td>
      <td>0</td>
    </tr>
    <tr>
      <th>406</th>
      <td>Task-generalizable Adversarial Attack based on Perceptual Metric</td>
      <td>0</td>
    </tr>
    <tr>
      <th>407</th>
      <td>Metric Attack and Defense for Person Re-identification</td>
      <td>0</td>
    </tr>
    <tr>
      <th>408</th>
      <td>Uncertainty Propagation in Deep Neural Network Using Active Subspace</td>
      <td>0</td>
    </tr>
    <tr>
      <th>409</th>
      <td>GanDef: A GAN based Adversarial Training Defense for Neural Network\n  Classifier</td>
      <td>0</td>
    </tr>
    <tr>
      <th>410</th>
      <td>Statistical Guarantees for the Robustness of Bayesian Neural Networks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>411</th>
      <td>Humans can decipher adversarial images</td>
      <td>0</td>
    </tr>
    <tr>
      <th>412</th>
      <td>PuVAE: A Variational Autoencoder to Purify Adversarial Examples</td>
      <td>0</td>
    </tr>
    <tr>
      <th>413</th>
      <td>Adaptive Gradient for Adversarial Perturbations Generation</td>
      <td>0</td>
    </tr>
    <tr>
      <th>414</th>
      <td>A Variational Dirichlet Framework for Out-of-Distribution Detection</td>
      <td>0</td>
    </tr>
    <tr>
      <th>415</th>
      <td>Adversarial attacks hidden in plain sight</td>
      <td>0</td>
    </tr>
    <tr>
      <th>416</th>
      <td>Can Intelligent Hyperparameter Selection Improve Resistance to\n  Adversarial Examples?</td>
      <td>0</td>
    </tr>
    <tr>
      <th>417</th>
      <td>When Causal Intervention Meets Image Masking and Adversarial\n  Perturbation for Deep Neural Net...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>418</th>
      <td>Generating Adversarial Perturbation with Root Mean Square Gradient</td>
      <td>0</td>
    </tr>
    <tr>
      <th>419</th>
      <td>Daedalus: Breaking Non-Maximum Suppression in Object Detection via\n  Adversarial Examples</td>
      <td>0</td>
    </tr>
    <tr>
      <th>420</th>
      <td>Who's Afraid of Adversarial Queries? The Impact of Image Modifications\n  on Content-based Image...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>421</th>
      <td>Using Pre-Training Can Improve Model Robustness and Uncertainty</td>
      <td>0</td>
    </tr>
    <tr>
      <th>422</th>
      <td>CapsAttacks: Robust and Imperceptible Adversarial Attacks on Capsule\n  Networks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>423</th>
      <td>A Noise-Sensitivity-Analysis-Based Test Prioritization Technique for\n  Deep Neural Networks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>424</th>
      <td>Adversarial Examples versus Cloud-based Detectors: A Black-box Empirical\n  Study</td>
      <td>0</td>
    </tr>
    <tr>
      <th>425</th>
      <td>Characterizing and evaluating adversarial examples for Offline\n  Handwritten Signature Verifica...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>426</th>
      <td>Image Transformation can make Neural Networks more robust against\n  Adversarial Examples</td>
      <td>0</td>
    </tr>
    <tr>
      <th>427</th>
      <td>Interpretable BoW Networks for Adversarial Example Detection</td>
      <td>0</td>
    </tr>
    <tr>
      <th>428</th>
      <td>Transferable Adversarial Attacks for Image and Video Object Detection</td>
      <td>0</td>
    </tr>
    <tr>
      <th>429</th>
      <td>DeepBillboard: Systematic Physical-World Testing of Autonomous Driving\n  Systems</td>
      <td>0</td>
    </tr>
    <tr>
      <th>430</th>
      <td>A Data-driven Adversarial Examples Recognition Framework via Adversarial\n  Feature Genome</td>
      <td>0</td>
    </tr>
    <tr>
      <th>431</th>
      <td>Detection based Defense against Adversarial Examples from the\n  Steganalysis Point of View</td>
      <td>0</td>
    </tr>
    <tr>
      <th>432</th>
      <td>Convolutional Neural Networks with Transformed Input based on Robust\n  Tensor Network Decomposi...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>433</th>
      <td>Optimal Transport Classifier: Defending Against Adversarial Attacks by\n  Regularized Deep Embed...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>434</th>
      <td>AutoGAN: Robust Classifier Against Adversarial Attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>435</th>
      <td>Detecting Adversarial Examples in Convolutional Neural Networks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>436</th>
      <td>Towards Leveraging the Information of Gradients in Optimization-based\n  Adversarial Attack</td>
      <td>0</td>
    </tr>
    <tr>
      <th>437</th>
      <td>Regularized Ensembles and Transferability in Adversarial Learning</td>
      <td>0</td>
    </tr>
    <tr>
      <th>438</th>
      <td>FineFool: Fine Object Contour Attack via Attention</td>
      <td>0</td>
    </tr>
    <tr>
      <th>439</th>
      <td>ComDefend: An Efficient Image Compression Model to Defend Adversarial\n  Examples</td>
      <td>0</td>
    </tr>
    <tr>
      <th>440</th>
      <td>Regularized adversarial examples for model interpretability</td>
      <td>0</td>
    </tr>
    <tr>
      <th>441</th>
      <td>Intermediate Level Adversarial Attack for Enhanced Transferability</td>
      <td>0</td>
    </tr>
    <tr>
      <th>442</th>
      <td>Classifiers Based on Deep Sparse Coding Architectures are Robust to Deep\n  Learning Transferabl...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>443</th>
      <td>Exploring Adversarial Examples: Patterns of One-Pixel Attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>444</th>
      <td>Emerging Applications of Reversible Data Hiding</td>
      <td>0</td>
    </tr>
    <tr>
      <th>445</th>
      <td>CAAD 2018: Iterative Ensemble Adversarial Attack</td>
      <td>0</td>
    </tr>
    <tr>
      <th>446</th>
      <td>Generalizing to Unseen Domains via Adversarial Data Augmentation</td>
      <td>0</td>
    </tr>
    <tr>
      <th>447</th>
      <td>FUNN: Flexible Unsupervised Neural Network</td>
      <td>0</td>
    </tr>
    <tr>
      <th>448</th>
      <td>Improved Network Robustness with Adversary Critic</td>
      <td>0</td>
    </tr>
    <tr>
      <th>449</th>
      <td>One Bit Matters: Understanding Adversarial Examples as the Abuse of\n  Redundancy</td>
      <td>0</td>
    </tr>
    <tr>
      <th>450</th>
      <td>Projecting Trouble: Light Based Adversarial Attacks on Deep Learning\n  Classifiers</td>
      <td>0</td>
    </tr>
    <tr>
      <th>451</th>
      <td>Unifying Bilateral Filtering and Adversarial Training for Robust Neural\n  Networks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>452</th>
      <td>Controlling Over-generalization and its Effect on Adversarial Examples\n  Generation and Detection</td>
      <td>0</td>
    </tr>
    <tr>
      <th>453</th>
      <td>On The Utility of Conditional Generation Based Mutual Information for\n  Characterizing Adversar...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>454</th>
      <td>Defensive Dropout for Hardening Deep Neural Networks under Adversarial\n  Attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>455</th>
      <td>Classification by Re-generation: Towards Classification Based on\n  Variational Inference</td>
      <td>0</td>
    </tr>
    <tr>
      <th>456</th>
      <td>Simultaneous Adversarial Training - Learn from Others Mistakes</td>
      <td>0</td>
    </tr>
    <tr>
      <th>457</th>
      <td>Adversarial Attack Type I: Generating False Positives</td>
      <td>0</td>
    </tr>
    <tr>
      <th>458</th>
      <td>Query-Free Attacks on Industry-Grade Face Recognition Systems under\n  Resource Constraints</td>
      <td>0</td>
    </tr>
    <tr>
      <th>459</th>
      <td>On the Robustness of Semantic Segmentation Models to Adversarial Attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>460</th>
      <td>Customizing an Adversarial Example Generator with Class-Conditional GANs</td>
      <td>0</td>
    </tr>
    <tr>
      <th>461</th>
      <td>Evaluation of Momentum Diverse Input Iterative Fast Gradient Sign Method\n  (M-DI2-FGSM) Based A...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>462</th>
      <td>Copycat CNN: Stealing Knowledge by Persuading Confession with Random\n  Non-Labeled Data</td>
      <td>0</td>
    </tr>
    <tr>
      <th>463</th>
      <td>ASP:A Fast Adversarial Attack Example Generation Framework based on\n  Adversarial Saliency Pred...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>464</th>
      <td>Adversarial Examples in Remote Sensing</td>
      <td>0</td>
    </tr>
    <tr>
      <th>465</th>
      <td>MAT: A Multi-strength Adversarial Training Method to Mitigate\n  Adversarial Attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>466</th>
      <td>On the Limitation of MagNet Defense against $L_1$-based Adversarial\n  Examples</td>
      <td>0</td>
    </tr>
    <tr>
      <th>467</th>
      <td>Defense against Adversarial Attacks Using High-Level Representation\n  Guided Denoiser</td>
      <td>0</td>
    </tr>
    <tr>
      <th>468</th>
      <td>Wasserstein Introspective Neural Networks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>469</th>
      <td>Query-Efficient Black-box Adversarial Examples (superceded)</td>
      <td>0</td>
    </tr>
    <tr>
      <th>470</th>
      <td>The Effects of JPEG and JPEG2000 Compression on Attacks using\n  Adversarial Examples</td>
      <td>0</td>
    </tr>
    <tr>
      <th>471</th>
      <td>Rethinking Feature Distribution for Loss Functions in Image\n  Classification</td>
      <td>0</td>
    </tr>
    <tr>
      <th>472</th>
      <td>Generalizable Adversarial Examples Detection Based on Bi-model Decision\n  Mismatch</td>
      <td>0</td>
    </tr>
    <tr>
      <th>473</th>
      <td>Neural Networks in Adversarial Setting and Ill-Conditioned Weight Space</td>
      <td>0</td>
    </tr>
    <tr>
      <th>474</th>
      <td>Virtual Adversarial Ladder Networks For Semi-supervised Learning</td>
      <td>0</td>
    </tr>
    <tr>
      <th>475</th>
      <td>Variational Inference with Latent Space Quantization for Adversarial\n  Resilience</td>
      <td>0</td>
    </tr>
    <tr>
      <th>476</th>
      <td>Improving Adversarial Robustness via Guided Complement Entropy</td>
      <td>0</td>
    </tr>
    <tr>
      <th>477</th>
      <td>Robust Image Segmentation Quality Assessment without Ground Truth</td>
      <td>0</td>
    </tr>
    <tr>
      <th>478</th>
      <td>Attack Type Agnostic Perceptual Enhancement of Adversarial Images</td>
      <td>0</td>
    </tr>
    <tr>
      <th>479</th>
      <td>Robustness of Generalized Learning Vector Quantization Models against\n  Adversarial Attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>480</th>
      <td>Cautious Deep Learning</td>
      <td>0</td>
    </tr>
    <tr>
      <th>481</th>
      <td>Disentangled Deep Autoencoding Regularization for Robust Image\n  Classification</td>
      <td>0</td>
    </tr>
    <tr>
      <th>482</th>
      <td>A Deep, Information-theoretic Framework for Robust Biometric Recognition</td>
      <td>0</td>
    </tr>
    <tr>
      <th>483</th>
      <td>Perceptual Quality-preserving Black-Box Attack against Deep Learning\n  Image Classifiers</td>
      <td>0</td>
    </tr>
    <tr>
      <th>484</th>
      <td>Extending Adversarial Attacks and Defenses to Deep 3D Point Cloud\n  Classifiers</td>
      <td>0</td>
    </tr>
    <tr>
      <th>485</th>
      <td>Image Decomposition and Classification through a Generative Model</td>
      <td>0</td>
    </tr>
    <tr>
      <th>486</th>
      <td>Implicit Generative Modeling of Random Noise during Training for\n  Adversarial Robustness</td>
      <td>0</td>
    </tr>
    <tr>
      <th>487</th>
      <td>Robustness Of Saak Transform Against Adversarial Attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>488</th>
      <td>Approximate Newton-based statistical inference using only stochastic\n  gradients</td>
      <td>0</td>
    </tr>
    <tr>
      <th>489</th>
      <td>Natural and Adversarial Error Detection using Invariance to Image\n  Transformations</td>
      <td>0</td>
    </tr>
    <tr>
      <th>490</th>
      <td>Improving Model Robustness with Transformation-Invariant Attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>491</th>
      <td>With Friends Like These, Who Needs Adversaries?</td>
      <td>0</td>
    </tr>
    <tr>
      <th>492</th>
      <td>Defense-VAE: A Fast and Accurate Defense against Adversarial Attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>493</th>
      <td>Adversarial Framing for Image and Video Classification</td>
      <td>0</td>
    </tr>
    <tr>
      <th>494</th>
      <td>Adversarial Defense of Image Classification Using a Variational\n  Auto-Encoder</td>
      <td>0</td>
    </tr>
    <tr>
      <th>495</th>
      <td>Towards Hiding Adversarial Examples from Network Interpretation</td>
      <td>0</td>
    </tr>
    <tr>
      <th>496</th>
      <td>SADA: Semantic Adversarial Diagnostic Attacks for Autonomous\n  Applications</td>
      <td>0</td>
    </tr>
    <tr>
      <th>497</th>
      <td>Attacks on State-of-the-Art Face Recognition using Attentional\n  Adversarial Attack Generative ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>498</th>
      <td>Adversarial Attacks for Optical Flow-Based Action Recognition\n  Classifiers</td>
      <td>0</td>
    </tr>
    <tr>
      <th>499</th>
      <td>Noisy Computations during Inference: Harmful or Helpful?</td>
      <td>0</td>
    </tr>
    <tr>
      <th>500</th>
      <td>Attention, Please! Adversarial Defense via Attention Rectification and\n  Preservation</td>
      <td>0</td>
    </tr>
    <tr>
      <th>501</th>
      <td>MimicGAN: Corruption-Mimicking for Blind Image Recovery &amp; Adversarial\n  Defense</td>
      <td>0</td>
    </tr>
    <tr>
      <th>502</th>
      <td>Local Gradients Smoothing: Defense against localized adversarial attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>503</th>
      <td>Analysis of adversarial attacks against CNN-based image forgery\n  detectors</td>
      <td>0</td>
    </tr>
    <tr>
      <th>504</th>
      <td>Gray-box Adversarial Training</td>
      <td>0</td>
    </tr>
    <tr>
      <th>505</th>
      <td>SSIMLayer: Towards Robust Deep Representation Learning via Nonlinear\n  Structural Similarity</td>
      <td>0</td>
    </tr>
    <tr>
      <th>506</th>
      <td>Adversarial Attacks on Variational Autoencoders</td>
      <td>0</td>
    </tr>
    <tr>
      <th>507</th>
      <td>Robust Blind Deconvolution via Mirror Descent</td>
      <td>0</td>
    </tr>
    <tr>
      <th>508</th>
      <td>Protecting JPEG Images Against Adversarial Attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>509</th>
      <td>Defense Against Adversarial Images using Web-Scale Nearest-Neighbor\n  Search</td>
      <td>0</td>
    </tr>
    <tr>
      <th>510</th>
      <td>A Kernelized Manifold Mapping to Diminish the Effect of Adversarial\n  Perturbations</td>
      <td>0</td>
    </tr>
    <tr>
      <th>511</th>
      <td>On the Effectiveness of Low Frequency Perturbations</td>
      <td>0</td>
    </tr>
    <tr>
      <th>512</th>
      <td>Data Fine-tuning</td>
      <td>0</td>
    </tr>
    <tr>
      <th>513</th>
      <td>Hessian-based Analysis of Large Batch Training and Robustness to\n  Adversaries</td>
      <td>0</td>
    </tr>
    <tr>
      <th>514</th>
      <td>Adversarial Defense by Stratified Convolutional Sparse Coding</td>
      <td>0</td>
    </tr>
    <tr>
      <th>515</th>
      <td>Exploring the Vulnerability of Single Shot Module in Object Detectors\n  via Imperceptible Backg...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>516</th>
      <td>A Deeper Look at 3D Shape Classifiers</td>
      <td>0</td>
    </tr>
    <tr>
      <th>517</th>
      <td>On the Structural Sensitivity of Deep Convolutional Networks to the\n  Directions of Fourier Bas...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>518</th>
      <td>Ask, Acquire, and Attack: Data-free UAP Generation using Class\n  Impressions</td>
      <td>0</td>
    </tr>
    <tr>
      <th>519</th>
      <td>Vulnerability Analysis of Chest X-Ray Image Classification Against\n  Adversarial Attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>520</th>
      <td>A general metric for identifying adversarial images</td>
      <td>0</td>
    </tr>
    <tr>
      <th>521</th>
      <td>Using LIP to Gloss Over Faces in Single-Stage Face Detection Networks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>522</th>
      <td>On Lyapunov exponents and adversarial perturbation</td>
      <td>0</td>
    </tr>
    <tr>
      <th>523</th>
      <td>Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>524</th>
      <td>A new Backdoor Attack in CNNs by training set corruption without label\n  poisoning</td>
      <td>0</td>
    </tr>
    <tr>
      <th>525</th>
      <td>LOGAN: Unpaired Shape Transform in Latent Overcomplete Space</td>
      <td>0</td>
    </tr>
    <tr>
      <th>526</th>
      <td>RoPAD: Robust Presentation Attack Detection through Unsupervised\n  Adversarial Invariance</td>
      <td>0</td>
    </tr>
    <tr>
      <th>527</th>
      <td>Inserting Videos into Videos</td>
      <td>0</td>
    </tr>
    <tr>
      <th>528</th>
      <td>Adversarial Attack and Defense on Point Sets</td>
      <td>0</td>
    </tr>
    <tr>
      <th>529</th>
      <td>advertorch v0.1: An Adversarial Robustness Toolbox based on PyTorch</td>
      <td>0</td>
    </tr>
    <tr>
      <th>530</th>
      <td>DeepFault: Fault Localization for Deep Neural Networks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>531</th>
      <td>Minimal Images in Deep Neural Networks: Fragile Object Recognition in\n  Natural Images</td>
      <td>0</td>
    </tr>
    <tr>
      <th>532</th>
      <td>Fooling Neural Network Interpretations via Adversarial Model\n  Manipulation</td>
      <td>0</td>
    </tr>
    <tr>
      <th>533</th>
      <td>Understanding the One-Pixel Attack: Propagation Maps and Locality\n  Analysis</td>
      <td>0</td>
    </tr>
    <tr>
      <th>534</th>
      <td>Disguised-Nets: Image Disguising for Privacy-preserving Deep Learning</td>
      <td>0</td>
    </tr>
    <tr>
      <th>535</th>
      <td>Image classification and retrieval with random depthwise signed\n  convolutional neural networks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>536</th>
      <td>Bilateral Adversarial Training: Towards Fast Training of More Robust\n  Models Against Adversari...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>537</th>
      <td>Improving Document Binarization via Adversarial Noise-Texture\n  Augmentation</td>
      <td>0</td>
    </tr>
    <tr>
      <th>538</th>
      <td>Dissociable neural representations of adversarially perturbed images in\n  deep neural networks ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>539</th>
      <td>Non-Adversarial Image Synthesis with Generative Latent Nearest Neighbors</td>
      <td>0</td>
    </tr>
    <tr>
      <th>540</th>
      <td>Unsupervised Adversarial Visual Level Domain Adaptation for Learning\n  Video Object Detectors f...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>541</th>
      <td>Recursive Chaining of Reversible Image-to-image Translators For Face\n  Aging</td>
      <td>0</td>
    </tr>
    <tr>
      <th>542</th>
      <td>Security Consideration For Deep Learning-Based Image Forensics</td>
      <td>0</td>
    </tr>
    <tr>
      <th>543</th>
      <td>Adversarial Image Alignment and Interpolation</td>
      <td>0</td>
    </tr>
    <tr>
      <th>544</th>
      <td>Vision-based Navigation of Autonomous Vehicle in Roadway Environments\n  with Unexpected Hazards</td>
      <td>0</td>
    </tr>
    <tr>
      <th>545</th>
      <td>Adversarial Sampling for Active Learning</td>
      <td>0</td>
    </tr>
    <tr>
      <th>546</th>
      <td>Towards Adversarial Training with Moderate Performance Improvement for\n  Neural Network Classif...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>547</th>
      <td>Versatile Auxiliary Classifier with Generative Adversarial Network\n  (VAC+GAN)</td>
      <td>0</td>
    </tr>
    <tr>
      <th>548</th>
      <td>Detecting Adversarial Samples Using Density Ratio Estimates</td>
      <td>0</td>
    </tr>
    <tr>
      <th>549</th>
      <td>JumpReLU: A Retrofit Defense Strategy for Adversarial Attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>550</th>
      <td>Bit-Flip Attack: Crushing Neural Network with Progressive Bit Search</td>
      <td>0</td>
    </tr>
    <tr>
      <th>551</th>
      <td>Evading Defenses to Transferable Adversarial Examples by\n  Translation-Invariant Attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>552</th>
      <td>Adversarial camera stickers: A physical camera-based attack on deep\n  learning systems</td>
      <td>0</td>
    </tr>
    <tr>
      <th>553</th>
      <td>Interpreting Adversarial Examples by Activation Promotion and\n  Suppression</td>
      <td>0</td>
    </tr>
    <tr>
      <th>554</th>
      <td>Curls &amp; Whey: Boosting Black-Box Adversarial Attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>555</th>
      <td>Adversarial Defense by Restricting the Hidden Space of Deep Neural\n  Networks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>556</th>
      <td>Adversarial Attacks against Deep Saliency Models</td>
      <td>0</td>
    </tr>
    <tr>
      <th>557</th>
      <td>Regional Homogeneity: Towards Learning Transferable Universal\n  Adversarial Perturbations Again...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>558</th>
      <td>Second Rethinking of Network Pruning in the Adversarial Setting</td>
      <td>0</td>
    </tr>
    <tr>
      <th>559</th>
      <td>Text Processing Like Humans Do: Visually Attacking and Shielding NLP\n  Systems</td>
      <td>0</td>
    </tr>
    <tr>
      <th>560</th>
      <td>Scaling up the randomized gradient-free adversarial attack reveals\n  overestimation of robustne...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>561</th>
      <td>A geometry-inspired decision-based attack</td>
      <td>0</td>
    </tr>
    <tr>
      <th>562</th>
      <td>Deep Co-Training for Semi-Supervised Image Segmentation</td>
      <td>0</td>
    </tr>
    <tr>
      <th>563</th>
      <td>Bridging Adversarial Robustness and Gradient Interpretability</td>
      <td>0</td>
    </tr>
    <tr>
      <th>564</th>
      <td>Calibration of Encoder Decoder Models for Neural Machine Translation</td>
      <td>0</td>
    </tr>
    <tr>
      <th>565</th>
      <td>Taking a HINT: Leveraging Explanations to Make Vision and Language\n  Models More Grounded</td>
      <td>0</td>
    </tr>
    <tr>
      <th>566</th>
      <td>Failing Loudly: An Empirical Study of Methods for Detecting Dataset\n  Shift</td>
      <td>0</td>
    </tr>
    <tr>
      <th>567</th>
      <td>Learning Robust Representations by Projecting Superficial Statistics Out</td>
      <td>0</td>
    </tr>
    <tr>
      <th>568</th>
      <td>A Statistical Approach to Assessing Neural Network Robustness</td>
      <td>0</td>
    </tr>
    <tr>
      <th>569</th>
      <td>Adversarial Attacks on Graph Neural Networks via Meta Learning</td>
      <td>0</td>
    </tr>
    <tr>
      <th>570</th>
      <td>Robust Neural Abstractive Summarization Systems and Evaluation against\n  Adversarial Information</td>
      <td>0</td>
    </tr>
    <tr>
      <th>571</th>
      <td>Are you tough enough? Framework for Robustness Validation of Machine\n  Comprehension Systems</td>
      <td>0</td>
    </tr>
    <tr>
      <th>572</th>
      <td>A Gray Box Interpretable Visual Debugging Approach for Deep Sequence\n  Learning Model</td>
      <td>0</td>
    </tr>
    <tr>
      <th>573</th>
      <td>CausalGAN: Learning Causal Implicit Generative Models with Adversarial\n  Training</td>
      <td>0</td>
    </tr>
    <tr>
      <th>574</th>
      <td>Are Generative Classifiers More Robust to Adversarial Attacks?</td>
      <td>0</td>
    </tr>
    <tr>
      <th>575</th>
      <td>Attack Graph Convolutional Networks by Adding Fake Nodes</td>
      <td>0</td>
    </tr>
    <tr>
      <th>576</th>
      <td>Laplacian Networks: Bounding Indicator Function Smoothness for Neural\n  Network Robustness</td>
      <td>0</td>
    </tr>
    <tr>
      <th>577</th>
      <td>Neural Networks with Structural Resistance to Adversarial Attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>578</th>
      <td>Efficient Decision-based Black-box Adversarial Attacks on Face\n  Recognition</td>
      <td>0</td>
    </tr>
    <tr>
      <th>579</th>
      <td>Smooth Adversarial Examples</td>
      <td>0</td>
    </tr>
    <tr>
      <th>580</th>
      <td>A New Angle on L2 Regularization</td>
      <td>0</td>
    </tr>
    <tr>
      <th>581</th>
      <td>Black-Box Decision based Adversarial Attack with Symmetric\n  $$-stable Distribution</td>
      <td>0</td>
    </tr>
    <tr>
      <th>582</th>
      <td>UniVSE: Robust Visual Semantic Embeddings via Structured Semantic\n  Representations</td>
      <td>0</td>
    </tr>
    <tr>
      <th>583</th>
      <td>ZK-GanDef: A GAN based Zero Knowledge Adversarial Training Defense for\n  Neural Networks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>584</th>
      <td>Semantic Adversarial Attacks: Parametric Transformations That Fool Deep\n  Classifiers</td>
      <td>0</td>
    </tr>
    <tr>
      <th>585</th>
      <td>Interpreting Adversarial Examples with Attributes</td>
      <td>0</td>
    </tr>
    <tr>
      <th>586</th>
      <td>AT-GAN: A Generative Attack Model for Adversarial Transferring on\n  Generative Adversarial Nets</td>
      <td>0</td>
    </tr>
    <tr>
      <th>587</th>
      <td>Big but Imperceptible Adversarial Perturbations via Semantic\n  Manipulation</td>
      <td>0</td>
    </tr>
    <tr>
      <th>588</th>
      <td>Cycle-Consistent Adversarial GAN: the integration of adversarial attack\n  and defense</td>
      <td>0</td>
    </tr>
    <tr>
      <th>589</th>
      <td>Fooling automated surveillance cameras: adversarial patches to attack\n  person detection</td>
      <td>0</td>
    </tr>
    <tr>
      <th>590</th>
      <td>Adversarial Defense Through Network Profiling Based Path Extraction</td>
      <td>0</td>
    </tr>
    <tr>
      <th>591</th>
      <td>Influence of Control Parameters and the Size of Biomedical Image\n  Datasets on the Success of A...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>592</th>
      <td>Evaluating Robustness of Deep Image Super-Resolution against Adversarial\n  Attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>593</th>
      <td>Using Videos to Evaluate Image Model Robustness</td>
      <td>0</td>
    </tr>
    <tr>
      <th>594</th>
      <td>Beyond Explainability: Leveraging Interpretability for Improved\n  Adversarial Learning</td>
      <td>0</td>
    </tr>
    <tr>
      <th>595</th>
      <td>Minimizing Perceived Image Quality Loss Through Adversarial Attack\n  Scoping</td>
      <td>0</td>
    </tr>
    <tr>
      <th>596</th>
      <td>blessing in disguise: Designing Robust Turing Test by Employing\n  Algorithm Unrobustness</td>
      <td>0</td>
    </tr>
    <tr>
      <th>597</th>
      <td>Salient Object Detection in the Deep Learning Era: An In-Depth Survey</td>
      <td>0</td>
    </tr>
    <tr>
      <th>598</th>
      <td>NATTACK: Learning the Distributions of Adversarial Examples for an\n  Improved Black-Box Attack ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>599</th>
      <td>Learning Interpretable Features via Adversarially Robust Optimization</td>
      <td>0</td>
    </tr>
    <tr>
      <th>600</th>
      <td>Adversarial Image Translation: Unrestricted Adversarial Examples in Face\n  Recognition Systems</td>
      <td>0</td>
    </tr>
    <tr>
      <th>601</th>
      <td>Representation of White- and Black-Box Adversarial Examples in Deep\n  Neural Networks and Human...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>602</th>
      <td>Adversarial Examples Are Not Bugs, They Are Features</td>
      <td>0</td>
    </tr>
    <tr>
      <th>603</th>
      <td>Better the Devil you Know: An Analysis of Evasion Attacks using\n  Out-of-Distribution Adversari...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>604</th>
      <td>Interpreting and Evaluating Neural Network Robustness</td>
      <td>0</td>
    </tr>
    <tr>
      <th>605</th>
      <td>On the Connection Between Adversarial Robustness and Saliency Map\n  Interpretability</td>
      <td>0</td>
    </tr>
    <tr>
      <th>606</th>
      <td>Exact Adversarial Attack to Image Captioning via Structured Output\n  Learning with Latent Varia...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>607</th>
      <td>ROSA: Robust Salient Object Detection against Adversarial Attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>608</th>
      <td>Non-Local Context Encoder: Robust Biomedical Image Segmentation against\n  Adversarial Attacks</td>
      <td>0</td>
    </tr>
    <tr>
      <th>609</th>
      <td>Generating Realistic Unrestricted Adversarial Inputs using\n  Dual-Objective GAN Training</td>
      <td>0</td>
    </tr>
    <tr>
      <th>610</th>
      <td>AnonymousNet: Natural Face De-Identification with Measurable Privacy</td>
      <td>0</td>
    </tr>
    <tr>
      <th>611</th>
      <td>Shallow-Deep Networks: Understanding and Mitigating Network Overthinking</td>
      <td>0</td>
    </tr>
    <tr>
      <th>612</th>
      <td>Towards Evaluating and Understanding Robust Optimisation under Transfer</td>
      <td>0</td>
    </tr>
    <tr>
      <th>613</th>
      <td>MixMatch: A Holistic Approach to Semi-Supervised Learning</td>
      <td>0</td>
    </tr>
  </tbody>
</table>